{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. LoRAによるファインチューニング (学習と永続化)\n",
    "\n",
    "このノートブックでは、LoRA（QLoRA）による学習を行い、その成果物を **Google Driveに保存** します。これにより、別の日に行われる統合演習（07）で、学習したモデルを確実に再利用できます。\n",
    "\n",
    "## 事前準備\n",
    "Google Colabのメニュー「ランタイム」→「ランタイムのタイプを変更」で **T4 GPU** を選択してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Google Driveのマウント (永続保存に必須)\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if os.path.exists(repo_path): !rm -rf {repo_path}\n",
    "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "os.chdir(repo_path)\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft trl datasets gradio\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.common import load_llm, generate_text\n",
    "from src.lora import create_lora_model, train_lora\n",
    "from peft import PeftModel\n",
    "print('セットアップが完了しました。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ベースモデルのロードと学習前の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer = load_llm(use_4bit=True)\n",
    "\n",
    "question = \"『星屑のメモリー』の主人公について教えて。\"\n",
    "instruction = \"回答は必ず以下のJSON形式で出力してください。\\n{ \\\"answer\\\": \\\"...\\\", \\\"confidence\\\": \\\"high|medium|low\\\" }\"\n",
    "prompt = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{question}\\n{instruction}\\n\\n### 応答:\\n\"\n",
    "\n",
    "print(\"--- 学習前の回答 ---\")\n",
    "res = generate_text(base_model, tokenizer, prompt, max_new_tokens=128)\n",
    "print(res.split(\"### 応答:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRA学習の実行 (成果をGoogle Driveへ保存)\n",
    "\n",
    "学習したアダプタを Google Drive の `llm_lab_outputs/demo_adapter` に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# 保存先パスを Google Drive 上に完全固定\n",
    "OUTPUT_BASE_DIR = \"/content/drive/MyDrive/llm_lab_outputs/demo_adapter\"\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"1. LoRAアダプタをモデルに追加中...\")\n",
    "lora_model = create_lora_model(base_model)\n",
    "\n",
    "print(\"2. 簡易学習を開始します（約3〜5分）...\")\n",
    "train_lora(\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
    "    output_dir=OUTPUT_BASE_DIR,\n",
    "    max_steps=40,\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "print(f\"学習完了。成果物は Google Drive に永続保存されました: {OUTPUT_BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 保存されたアダプタのロードテスト\n",
    "\n",
    "セッションが切れた後の状態を想定し、**Google Drive から直接** アダプタをロードして動作を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "FINAL_ADAPTER_PATH = os.path.join(OUTPUT_BASE_DIR, \"final_adapter\")\n",
    "\n",
    "print(f\"Google Driveからアダプタをロードしています: {FINAL_ADAPTER_PATH}\")\n",
    "test_model = PeftModel.from_pretrained(base_model, FINAL_ADAPTER_PATH)\n",
    "test_model.eval()\n",
    "\n",
    "print(\"--- 学習後の回答 (Google Driveからロード) ---\")\n",
    "res = generate_text(test_model, tokenizer, prompt, max_new_tokens=128)\n",
    "print(res.split(\"### 応答:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "Google Driveに保存されたアダプタは、次回の **07. 統合デモ** でそのまま読み込んで使用できます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.12" }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
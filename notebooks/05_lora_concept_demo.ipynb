{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. LoRAによるファインチューニング (学習と永続化)\n",
        "\n",
        "このノートブックでは、LoRA（QLoRA）による軽量ファインチューニングを行い、その成果物を **Google Driveに実験単位で保存** します。保存したアダプタは **06. 統合演習** で再利用します。\n",
        "\n",
        "## 事前準備\n",
        "Google Colabのメニュー「ランタイム」→「ランタイムのタイプを変更」で **T4 GPU** を選択してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # CUDAデバッグ同期実行\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Google Driveのマウント\n",
        "if not os.path.isdir('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "repo_path = '/content/llm_lab'\n",
        "if os.path.exists(repo_path):\n",
        "    !rm -rf {repo_path}\n",
        "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
        "os.chdir(repo_path)\n",
        "\n",
        "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft datasets gradio\n",
        "if 'src' not in sys.path:\n",
        "    sys.path.append(os.path.abspath('src'))\n",
        "\n",
        "from src.common import load_llm, generate_text\n",
        "from src.lora import create_lora_model, train_lora\n",
        "from peft import PeftModel\n",
        "\n",
        "# 05/06で共有する保存先\n",
        "DRIVE_BASE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
        "DRIVE_RUNS_DIR = os.path.join(DRIVE_BASE_DIR, 'lora_runs')\n",
        "SELECTED_ADAPTER_RECORD_PATH = os.path.join(DRIVE_BASE_DIR, 'selected_adapter_path.txt')\n",
        "os.makedirs(DRIVE_RUNS_DIR, exist_ok=True)\n",
        "\n",
        "print('セットアップが完了しました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ベースモデルのロードと学習前の確認\n",
        "\n",
        "単一の質問だけでなく、複数の入力パターンで学習前の挙動を確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "base_model, tokenizer = load_llm(use_4bit=True)\n",
        "\n",
        "\n",
        "def normalize_special_tokens(tokenizer, model):\n",
        "    added = 0\n",
        "\n",
        "    # eosは既存を優先し、未設定時のみ追加\n",
        "    if tokenizer.eos_token is None:\n",
        "        added += tokenizer.add_special_tokens({'eos_token': '<|eos|>'})\n",
        "\n",
        "    # bosは未設定ならeosに合わせる\n",
        "    if tokenizer.bos_token is None:\n",
        "        tokenizer.bos_token = tokenizer.eos_token\n",
        "\n",
        "    # padはeosと分離する\n",
        "    if tokenizer.pad_token is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
        "        if tokenizer.unk_token is not None and tokenizer.unk_token_id != tokenizer.eos_token_id:\n",
        "            tokenizer.pad_token = tokenizer.unk_token\n",
        "            tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "        else:\n",
        "            added += tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
        "\n",
        "    # unkもeosと分離する\n",
        "    if tokenizer.unk_token is None or tokenizer.unk_token_id == tokenizer.eos_token_id:\n",
        "        added += tokenizer.add_special_tokens({'unk_token': '<|unk|>'})\n",
        "\n",
        "    if added > 0:\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    print('bos:', tokenizer.bos_token, tokenizer.bos_token_id)\n",
        "    print('eos:', tokenizer.eos_token, tokenizer.eos_token_id)\n",
        "    print('pad:', tokenizer.pad_token, tokenizer.pad_token_id)\n",
        "    print('unk:', tokenizer.unk_token, tokenizer.unk_token_id)\n",
        "\n",
        "    assert tokenizer.pad_token_id != tokenizer.eos_token_id, 'pad/eos が未分離です'\n",
        "    assert tokenizer.unk_token_id != tokenizer.eos_token_id, 'unk/eos が未分離です'\n",
        "\n",
        "\n",
        "normalize_special_tokens(tokenizer, base_model)\n",
        "\n",
        "\n",
        "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, temperature=0.7, top_p=0.9, repetition_penalty=1.05, do_sample=True):\n",
        "    \"\"\"05ノート専用: pipelineを使わずに安全側でgenerateする。\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        model_device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
        "\n",
        "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=False,\n",
        "            )\n",
        "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "def build_prompt(question: str, instruction: str = ''):\n",
        "    body = question if not instruction else f\"{question}\\n{instruction}\"\n",
        "    return (\n",
        "        '以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n'\n",
        "        f'### 指示:\\n{body}\\n\\n### 応答:\\n'\n",
        "    )\n",
        "\n",
        "\n",
        "DEMO_CASES = [\n",
        "    {\n",
        "        'name': 'JSON形式: 主人公情報',\n",
        "        'question': '『星屑のメモリー』の主人公について教えて。',\n",
        "        'instruction': '回答は必ず以下のJSON形式で出力してください。\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
        "    },\n",
        "    {\n",
        "        'name': 'JSON形式: あらすじ要約',\n",
        "        'question': '『古都の探偵録』のあらすじを2文で教えて。',\n",
        "        'instruction': '回答は必ず以下のJSON形式で出力してください。\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
        "    },\n",
        "    {\n",
        "        'name': '要約制約: 2文以内',\n",
        "        'question': '『最後の航海』の事件の概要を教えて。',\n",
        "        'instruction': '回答は2文以内で、固有名詞を1つ以上含めてください。',\n",
        "    },\n",
        "    {\n",
        "        'name': '通常QA: 作品ジャンル',\n",
        "        'question': '『シャドウ・ハンター』はどのようなジャンルの作品ですか？',\n",
        "        'instruction': '',\n",
        "    },\n",
        "    {\n",
        "        'name': '知識外ケース: 安全応答',\n",
        "        'question': '『銀河鉄道999』の2026年版アニメ映画の公式公開日は？',\n",
        "        'instruction': '根拠がない場合は推測せず、「分からない」と明記してください。',\n",
        "    },\n",
        "]\n",
        "\n",
        "# 学習の安定性を優先し、学習前生成はデフォルトでOFF\n",
        "RUN_PRECHECK_BEFORE_TRAIN = False\n",
        "\n",
        "if RUN_PRECHECK_BEFORE_TRAIN:\n",
        "    print('--- 学習前の回答（複数ケース）---')\n",
        "    for i, case in enumerate(DEMO_CASES, start=1):\n",
        "        prompt = build_prompt(case['question'], case['instruction'])\n",
        "        res = safe_generate_local(base_model, tokenizer, prompt, max_new_tokens=128)\n",
        "        answer = res.split('### 応答:')[-1].strip()\n",
        "        print(f\"[{i}] {case['name']}\")\n",
        "        print(f\"Q: {case['question']}\")\n",
        "        print(f\"A: {answer}\")\n",
        "        print('-' * 40)\n",
        "else:\n",
        "    print('学習前生成はスキップします（RUN_PRECHECK_BEFORE_TRAIN=False）。')\n",
        "    print('このまま学習セルへ進んでください。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LoRA学習の実行 (成果をGoogle Driveへ保存)\n",
        "\n",
        "`PROFILE_NAME` を切り替えて、計算負荷と学習効果のバランスを比較できます。\n",
        "\n",
        "- `quick`: 最短で完走確認（推奨）\n",
        "- `standard`: 演習向け標準設定\n",
        "- `extended`: 時間に余裕がある場合\n",
        "\n",
        "学習成果物は `llm_lab_outputs/lora_runs/<run_name>/final_adapter` に保存され、最新実験のパスは `llm_lab_outputs/selected_adapter_path.txt` に記録されます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "# 実験プロファイル（必要に応じて PROFILE_NAME を変更）\n",
        "PROFILE_NAME = 'quick'  # quick | standard | extended\n",
        "\n",
        "PROFILES = {\n",
        "    'quick': {\n",
        "        'max_steps': 10,\n",
        "        'per_device_train_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 4,\n",
        "        'max_seq_length': 256,\n",
        "        'learning_rate': 5e-5,\n",
        "    },\n",
        "    'standard': {\n",
        "        'max_steps': 30,\n",
        "        'per_device_train_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 8,\n",
        "        'max_seq_length': 512,\n",
        "        'learning_rate': 5e-5,\n",
        "    },\n",
        "    'extended': {\n",
        "        'max_steps': 60,\n",
        "        'per_device_train_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 8,\n",
        "        'max_seq_length': 512,\n",
        "        'learning_rate': 5e-5,\n",
        "    },\n",
        "}\n",
        "\n",
        "if PROFILE_NAME not in PROFILES:\n",
        "    raise ValueError(f'PROFILE_NAME must be one of: {list(PROFILES.keys())}')\n",
        "\n",
        "profile = PROFILES[PROFILE_NAME]\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "RUN_NAME = f\"{PROFILE_NAME}_s{profile['max_steps']}_lr{profile['learning_rate']}_{timestamp}\"\n",
        "DRIVE_OUTPUT_DIR = os.path.join(DRIVE_RUNS_DIR, RUN_NAME)\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f'Run name: {RUN_NAME}')\n",
        "print(f'Output dir: {DRIVE_OUTPUT_DIR}')\n",
        "\n",
        "print('1. LoRAアダプタをモデルに追加中...')\n",
        "lora_model = create_lora_model(base_model)\n",
        "\n",
        "print('2. 学習を開始します...')\n",
        "train_lora(\n",
        "    model=lora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
        "    output_dir=DRIVE_OUTPUT_DIR,\n",
        "    max_steps=profile['max_steps'],\n",
        "    learning_rate=profile['learning_rate'],\n",
        "    per_device_train_batch_size=profile['per_device_train_batch_size'],\n",
        "    gradient_accumulation_steps=profile['gradient_accumulation_steps'],\n",
        "    max_seq_length=profile['max_seq_length'],\n",
        ")\n",
        "\n",
        "FINAL_ADAPTER_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'final_adapter')\n",
        "RUN_CONFIG_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'run_config.json')\n",
        "\n",
        "run_config = {\n",
        "    'run_name': RUN_NAME,\n",
        "    'profile_name': PROFILE_NAME,\n",
        "    'train_dataset_path': 'data/lora/lora_train_sample.jsonl',\n",
        "    'final_adapter_path': FINAL_ADAPTER_PATH,\n",
        "    'params': profile,\n",
        "}\n",
        "\n",
        "with open(RUN_CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(run_config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(SELECTED_ADAPTER_RECORD_PATH, 'w', encoding='utf-8') as f:\n",
        "    f.write(FINAL_ADAPTER_PATH + '\\n')\n",
        "\n",
        "print(f'学習完了。アダプタ: {FINAL_ADAPTER_PATH}')\n",
        "print(f'設定保存: {RUN_CONFIG_PATH}')\n",
        "print(f'06向けアダプタ記録: {SELECTED_ADAPTER_RECORD_PATH}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 保存されたアダプタのロードテストと簡易評価\n",
        "\n",
        "Google Drive からアダプタをロードし、動作確認を行います。あわせて少数の評価質問で「JSON形式で返せているか」を簡易チェックします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "# アダプタの保存場所（通常は直前セルで生成）\n",
        "DRIVE_ADAPTER_PATH = globals().get('FINAL_ADAPTER_PATH', None)\n",
        "\n",
        "# 念のため selected_adapter_path.txt からも復元可能にする\n",
        "if not DRIVE_ADAPTER_PATH and os.path.exists(SELECTED_ADAPTER_RECORD_PATH):\n",
        "    with open(SELECTED_ADAPTER_RECORD_PATH, 'r', encoding='utf-8') as f:\n",
        "        DRIVE_ADAPTER_PATH = f.read().strip()\n",
        "\n",
        "\n",
        "def get_adapter_vocab_size(adapter_path: str):\n",
        "    \"\"\"アダプタに保存された埋め込み行数（語彙サイズ）を取得する。\"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "    state = None\n",
        "\n",
        "    safe_path = os.path.join(adapter_path, 'adapter_model.safetensors')\n",
        "    bin_path = os.path.join(adapter_path, 'adapter_model.bin')\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(safe_path):\n",
        "            from safetensors.torch import load_file\n",
        "            state = load_file(safe_path, device='cpu')\n",
        "        elif os.path.exists(bin_path):\n",
        "            state = torch.load(bin_path, map_location='cpu')\n",
        "    except Exception as e:\n",
        "        print(f'アダプタ語彙サイズの読み取りに失敗: {e}')\n",
        "        return None\n",
        "\n",
        "    if state is None:\n",
        "        return None\n",
        "\n",
        "    candidate_keys = [\n",
        "        'base_model.model.model.embed_tokens.weight',\n",
        "        'base_model.model.embed_tokens.weight',\n",
        "        'base_model.model.lm_head.weight',\n",
        "        'base_model.lm_head.weight',\n",
        "    ]\n",
        "\n",
        "    for k in candidate_keys:\n",
        "        if k in state and hasattr(state[k], 'shape') and len(state[k].shape) >= 2:\n",
        "            return int(state[k].shape[0])\n",
        "\n",
        "    for k, v in state.items():\n",
        "        if 'embed_tokens.weight' in k and hasattr(v, 'shape') and len(v.shape) >= 2:\n",
        "            return int(v.shape[0])\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def align_model_tokenizer_to_adapter(model, tokenizer, adapter_vocab_size):\n",
        "    \"\"\"推測でなくアダプタ期待サイズに合わせる。\"\"\"\n",
        "    model_vocab = model.get_input_embeddings().weight.shape[0]\n",
        "    tok_vocab = len(tokenizer)\n",
        "    print(f'Before align - Tokenizer: {tok_vocab}, Model: {model_vocab}, Adapter: {adapter_vocab_size}')\n",
        "\n",
        "    # tokenizerが小さい場合のみ埋める（大きい場合は増やさない）\n",
        "    if adapter_vocab_size is not None and tok_vocab < adapter_vocab_size:\n",
        "        add_n = adapter_vocab_size - tok_vocab\n",
        "        extras = [f'<|extra_{i}|>' for i in range(add_n)]\n",
        "        tokenizer.add_special_tokens({'additional_special_tokens': extras})\n",
        "        tok_vocab = len(tokenizer)\n",
        "        print(f'Tokenizer expanded to: {tok_vocab}')\n",
        "\n",
        "    # pad/eosの衝突回避（トークン追加せずID付け替え優先）\n",
        "    if tokenizer.eos_token_id is None:\n",
        "        tokenizer.eos_token_id = 0\n",
        "        tokenizer.eos_token = tokenizer.convert_ids_to_tokens(0)\n",
        "\n",
        "    if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
        "        if tokenizer.unk_token_id is not None and tokenizer.unk_token_id != tokenizer.eos_token_id:\n",
        "            tokenizer.pad_token = tokenizer.unk_token\n",
        "            tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "        else:\n",
        "            fallback_id = 1 if tokenizer.eos_token_id == 0 else 0\n",
        "            tokenizer.pad_token_id = fallback_id\n",
        "            tokenizer.pad_token = tokenizer.convert_ids_to_tokens(fallback_id)\n",
        "\n",
        "    # 最重要: モデル語彙サイズをアダプタ期待値へ合わせる\n",
        "    if adapter_vocab_size is not None:\n",
        "        if model_vocab != adapter_vocab_size:\n",
        "            model.resize_token_embeddings(adapter_vocab_size)\n",
        "            print(f'Model resized to adapter vocab: {adapter_vocab_size}')\n",
        "    else:\n",
        "        # アダプタから読み取れない場合のみtokenizerに合わせる\n",
        "        if model_vocab != tok_vocab:\n",
        "            model.resize_token_embeddings(tok_vocab)\n",
        "            print(f'Model resized to tokenizer vocab: {tok_vocab}')\n",
        "\n",
        "    print(f\"After align - eos_id: {tokenizer.eos_token_id}, pad_id: {tokenizer.pad_token_id}\")\n",
        "    print(f\"After align - Model vocab: {model.get_input_embeddings().weight.shape[0]}, Tokenizer vocab: {len(tokenizer)}\")\n",
        "\n",
        "\n",
        "def debug_generate(model, tokenizer, prompt, max_new_tokens=128):\n",
        "    \"\"\"詳細デバッグ版：どこでエラーが起きるか特定\"\"\"\n",
        "    try:\n",
        "        print('\\n[Debug] start')\n",
        "        print(f\"Prompt head: {prompt[:100]}...\")\n",
        "\n",
        "        print('Step 1: Tokenizing...')\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        print(f\"  input_ids shape: {inputs['input_ids'].shape}\")\n",
        "        print(f\"  input_ids range: [{inputs['input_ids'].min()}, {inputs['input_ids'].max()}]\")\n",
        "\n",
        "        print('Step 2: Moving to device...')\n",
        "        model_device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
        "        print(f'  device: {model_device}')\n",
        "\n",
        "        print('Step 3: Checking vocab size...')\n",
        "        vocab_size = model.get_input_embeddings().weight.shape[0]\n",
        "        max_id = inputs['input_ids'].max().item()\n",
        "        print(f'  vocab size: {vocab_size}')\n",
        "        print(f'  max input id: {max_id}')\n",
        "\n",
        "        if max_id >= vocab_size:\n",
        "            print(f'  WARNING: Token ID {max_id} >= vocab_size {vocab_size}')\n",
        "            inputs['input_ids'] = torch.clamp(inputs['input_ids'], 0, vocab_size - 1)\n",
        "            print(f'  clamped to range [0, {vocab_size - 1}]')\n",
        "\n",
        "        prompt_len = inputs['input_ids'].shape[1]\n",
        "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "        print(f'Step 4: Generating (max_new_tokens={max_new_tokens})...')\n",
        "        print(f'  pad_token_id: {pad_token_id}')\n",
        "        print(f'  eos_token_id: {tokenizer.eos_token_id}')\n",
        "\n",
        "        if hasattr(model, 'config'):\n",
        "            model.config.use_cache = False\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            print('  calling model.generate()...')\n",
        "            gen_ids = model.generate(\n",
        "                input_ids=inputs['input_ids'],\n",
        "                attention_mask=inputs.get('attention_mask'),\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=False,\n",
        "                do_sample=False\n",
        "            )\n",
        "            print('  generation completed')\n",
        "\n",
        "        print('Step 5: Decoding...')\n",
        "        new_ids = gen_ids[0][prompt_len:]\n",
        "        print(f'  generated ids shape: {new_ids.shape}')\n",
        "        if new_ids.numel() > 0:\n",
        "            print(f'  generated ids range: [{new_ids.min()}, {new_ids.max()}]')\n",
        "        else:\n",
        "            print('  generated ids are empty')\n",
        "\n",
        "        text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
        "        print(f'  decoded text length: {len(text)}')\n",
        "\n",
        "        return text if text else '[Empty output]'\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print('\\n[Debug] exception caught:')\n",
        "        print(f'Error type: {type(e).__name__}')\n",
        "        print(f'Error message: {str(e)}')\n",
        "        print('\\nFull traceback:')\n",
        "        traceback.print_exc()\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "if DRIVE_ADAPTER_PATH and os.path.exists(DRIVE_ADAPTER_PATH):\n",
        "    print(f'Google Driveからアダプタをロードしています: {DRIVE_ADAPTER_PATH}')\n",
        "\n",
        "    # クリーンアップ\n",
        "    for name in ('lora_model', 'base_model'):\n",
        "        if name in globals() and globals()[name] is not None:\n",
        "            del globals()[name]\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    eval_base_model, eval_tokenizer = load_llm(use_4bit=True)\n",
        "\n",
        "    adapter_vocab_size = get_adapter_vocab_size(DRIVE_ADAPTER_PATH)\n",
        "    print(f'Adapter vocab size: {adapter_vocab_size}')\n",
        "    align_model_tokenizer_to_adapter(eval_base_model, eval_tokenizer, adapter_vocab_size)\n",
        "\n",
        "    # LoRAロード\n",
        "    test_model = PeftModel.from_pretrained(eval_base_model, DRIVE_ADAPTER_PATH)\n",
        "    test_model.eval()\n",
        "\n",
        "    print('--- 学習後の回答（複数ケース）---')\n",
        "    for i, case in enumerate(DEMO_CASES, start=1):\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"[{i}] {case['name']}\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        prompt = build_prompt(case['question'], case['instruction'])\n",
        "        answer = debug_generate(test_model, eval_tokenizer, prompt, max_new_tokens=128)\n",
        "        print(f\"\\nFinal Answer: {answer}\")\n",
        "        print('-' * 60)\n",
        "\n",
        "        if 'Error:' in answer:\n",
        "            print('\\nエラーが発生したため、ここで停止します。')\n",
        "            break\n",
        "else:\n",
        "    print('エラー: Google Drive にアダプタが見つかりません。学習が正常に完了したか確認してください。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "- LoRAアダプタは `llm_lab_outputs/lora_runs/<run_name>/final_adapter` に保存されます。\n",
        "- 直近で使うアダプタは `llm_lab_outputs/selected_adapter_path.txt` に記録されます。\n",
        "- 次の **06. 統合演習** では、この記録ファイルを使ってアダプタを再利用します。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

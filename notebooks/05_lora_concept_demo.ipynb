{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. LoRAによるファインチューニング (概念デモ)\n",
    "\n",
    "RAGがLLMに「外部知識」を与える技術だったのに対し、**ファインチューニング**はLLM自体の「振る舞い」や「応答スタイル」を特定のタスクに合わせて適応させる技術です。\n",
    "\n",
    "しかし、巨大なLLM全体を再学習させるのは、膨大な計算資源（GPUメモリ、時間）が必要となり、非常にコストがかかります。\n",
    "\n",
    "そこで登場するのが **LoRA (Low-Rank Adaptation)** という、**PEFT (Parameter-Efficient Fine-Tuning)** の一種です。\n",
    "LoRAは、元のLLMの重みは固定したまま、新たに追加したごく一部の小さな重み（アダプタ）だけを学習します。これにより、少ない計算資源で効率的にファインチューニングを行うことができます。\n",
    "\n",
    "このノートブックでは、**特定の出力形式（JSON）を遵守するようにLoRAで事前学習されたアダプタ**を読み込み、その効果を体験します。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "`00_setup_common.ipynb` を実行して、ライブラリのインストールと設定が完了していることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# セットアップの確認と共通モジュールのインポート\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path): print('リポジトリが見つかりません。')\n",
    "else: os.chdir(repo_path)\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "try:\n",
    "    from src.common import load_llm, generate_text\n",
    "    from src.lora import load_lora_adapter\n",
    "    from peft import PeftModel\n",
    "    print('共通モジュールのインポートが完了しました。')\n",
    "except ImportError as e:\n",
    "    print(f'共通モジュールのインポートに失敗しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRAアダプタの準備\n",
    "\n",
    "この演習では、あらかじめ用意された学習済みLoRAアダプタを使用します。\n",
    "このアダプタは、`data/lora/lora_train_sample.jsonl` のようなデータセット（質問と、特定のJSON形式の回答のペア）を使って学習された、という想定です。\n",
    "\n",
    "（実際のアダプタは `06_lora_qlora_exercise.ipynb` で作成しますが、ここでは概念を理解するため、学習済みのものが既にあるとして進めます）\n",
    "\n",
    "まず、ダミーの学習済みアダプタをHugging Face Hubからダウンロードします。\n",
    "これは、`stabilityai/japanese-stablelm-3b-4e1t-instruct` に対して、JSON出力をするようにLoRAでチューニングされたモデルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "from src.lora import create_lora_model, train_lora\n",
    "\n",
    "adapter_path = \"./lora_adapters/demo_adapter\"\n",
    "\n",
    "# デモ用アダプタが存在しない場合、簡易学習を行って作成する\n",
    "if not os.path.exists(adapter_path):\n",
    "    print('デモ用アダプタを作成します（これには数分かかります）...')\n",
    "    \n",
    "    # 1. 学習用モデルの一時ロード\n",
    "    try:\n",
    "        tmp_model, tmp_tokenizer = load_llm(use_4bit=True)\n",
    "        \n",
    "        # 2. LoRAモデルの作成\n",
    "        tmp_lora_model = create_lora_model(tmp_model)\n",
    "        \n",
    "        # 3. 簡易学習の実行 (data/lora/lora_train_sample.jsonl を使用)\n",
    "        # デモ用なのでステップ数を少なく設定\n",
    "        train_lora(\n",
    "            model=tmp_lora_model,\n",
    "            tokenizer=tmp_tokenizer,\n",
    "            train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
    "            output_dir=adapter_path,\n",
    "            max_steps=30,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-4\n",
    "        )\n",
    "        \n",
    "        # 4. パスの調整 (train_loraは output_dir/final_adapter に保存するため)\n",
    "        if os.path.exists(os.path.join(adapter_path, \"final_adapter\")):\n",
    "            adapter_path = os.path.join(adapter_path, \"final_adapter\")\n",
    "            \n",
    "        # 5. メモリ解放\n",
    "        del tmp_model, tmp_lora_model, tmp_tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        print('デモ用アダプタの作成が完了しました。')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'アダプタ作成中にエラーが発生しました: {e}')\n",
    "else:\n",
    "    print('デモ用アダプタは既に存在します。')\n",
    "    if os.path.exists(os.path.join(adapter_path, \"final_adapter\")):\n",
    "        adapter_path = os.path.join(adapter_path, \"final_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベースモデルのロード\n",
    "\n",
    "LoRAアダプタを適用する前の、素のLLM（ベースモデル）をロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer = None, None\n",
    "try:\n",
    "    # LoRAアダプタを後から適用するため、ここでは `load_llm` を使わずに直接ロードします\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "    from src.common import DEFAULT_MODEL_ID\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        DEFAULT_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL_ID)\n",
    "    \n",
    "    print(\"ベースモデルのロードが完了しました。\")\n",
    "except Exception as e:\n",
    "    print(f'モデルのロード中にエラーが発生しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA適用前 vs 適用後 の比較\n",
    "\n",
    "同じプロンプトを使って、LoRAアダプタを適用する前と後で、LLMの応答がどのように変わるかを見てみましょう。\n",
    "プロンプトでは、応答をJSON形式にするように指示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "question = \"『星屑のメモリー』の主人公について教えて。\"\n",
    "instruction = \"回答は必ず以下のJSON形式で出力してください。\\n{ \\\"answer\\\": \\\"...\\\", \\\"confidence\\\": \\\"high|medium|low\\\" }\"\n",
    "\n",
    "prompt = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{question}\\n{instruction}\\n\\n### 応答:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース1: LoRA適用前 (ベースモデル)\n",
    "\n",
    "プロンプトでJSON形式を指示しても、必ずしもその通りに出力してくれるとは限りません。しばしば、形式が崩れたり、余計なテキストが付加されたりします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "if base_model and tokenizer:\n",
    "    print(\"--- LoRA適用前の回答 ---\")\n",
    "    generated_text = generate_text(base_model, tokenizer, prompt, max_new_tokens=128)\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース2: LoRA適用後\n",
    "\n",
    "次に、ベースモデルに学習済みLoRAアダプタを適用します。\n",
    "これにより、モデルはJSON形式で出力する「振る舞い」を学習しているため、指示に忠実に従うようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "lora_model = None\n",
    "try:\n",
    "    # ベースモデルにLoRAアダプタをロード\n",
    "    lora_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    lora_model.eval()\n",
    "    print(\"LoRAアダプタの適用が完了しました。\")\n",
    "except Exception as e:\n",
    "    print(f'LoRAアダプタの適用中にエラーが発生しました: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "if lora_model and tokenizer:\n",
    "    print(\"--- LoRA適用後の回答 ---\")\n",
    "    generated_text = generate_text(lora_model, tokenizer, prompt, max_new_tokens=128)\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"LoRAモデルが準備できていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、LoRAによるファインチューニングの概念とその効果を学びました。\n",
    "\n",
    "- **LoRAの効果**: 特定のタスク（今回はJSON形式での出力）に合わせてLLMの振る舞いを変化させることができる。\n",
    "- **RAGとの違い**: RAGが「知識」を外部から与えるのに対し、LoRAはLLM自体の「スキル」や「性格」を変化させるイメージ。\n",
    "\n",
    "LoRAは、応答スタイルの統一、特定の専門タスクへの特化、不適切な表現の抑制など、様々な目的に利用できます。\n",
    "\n",
    "今回は学習済みのアダプタを使いましたが、次の `06_lora_qlora_exercise.ipynb` では、いよいよ自分たちの手でQLoRA（4bit量子化モデルに対するLoRA）の学習を行います。\n",
    "\n",
    "### メモリ解放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import gc\n",
    "if 'base_model' in locals() and base_model is not None: del base_model\n",
    "if 'lora_model' in locals() and lora_model is not None: del lora_model\n",
    "if 'tokenizer' in locals() and tokenizer is not None: del tokenizer\n",
    "base_model, lora_model, tokenizer = None, None, None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"モデルを解放し、GPUキャッシュをクリアしました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

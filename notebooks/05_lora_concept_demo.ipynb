{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. LoRAによるファインチューニング (学習と永続化)\n",
    "\n",
    "このノートブックでは、LoRA（QLoRA）による軽量ファインチューニングを行い、その成果物を **Google Driveに実験単位で保存** します。保存したアダプタは **06. 統合演習** で再利用します。\n",
    "\n",
    "## 事前準備\n",
    "Google Colabのメニュー「ランタイム」→「ランタイムのタイプを変更」で **T4 GPU** を選択してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # CUDAデバッグ同期実行\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Google Driveのマウント\n",
    "if not os.path.isdir('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if os.path.exists(repo_path):\n",
    "    !rm -rf {repo_path}\n",
    "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "os.chdir(repo_path)\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft datasets gradio\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.common import load_llm, generate_text\n",
    "from src.lora import create_lora_model, train_lora\n",
    "from peft import PeftModel\n",
    "\n",
    "# 05/06で共有する保存先\n",
    "DRIVE_BASE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
    "DRIVE_RUNS_DIR = os.path.join(DRIVE_BASE_DIR, 'lora_runs')\n",
    "SELECTED_ADAPTER_RECORD_PATH = os.path.join(DRIVE_BASE_DIR, 'selected_adapter_path.txt')\n",
    "os.makedirs(DRIVE_RUNS_DIR, exist_ok=True)\n",
    "\n",
    "print('セットアップが完了しました。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ベースモデルのロードと学習前の確認\n",
    "\n",
    "単一の質問だけでなく、複数の入力パターンで学習前の挙動を確認します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer = load_llm(use_4bit=True)\n",
    "\n",
    "\n",
    "def normalize_special_tokens(tokenizer, model):\n",
    "    added = 0\n",
    "\n",
    "    # eosは既存を優先し、未設定時のみ追加\n",
    "    if tokenizer.eos_token is None:\n",
    "        added += tokenizer.add_special_tokens({'eos_token': '<|eos|>'})\n",
    "\n",
    "    # bosは未設定ならeosに合わせる\n",
    "    if tokenizer.bos_token is None:\n",
    "        tokenizer.bos_token = tokenizer.eos_token\n",
    "\n",
    "    # padはeosと分離する\n",
    "    if tokenizer.pad_token is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "        if tokenizer.unk_token is not None and tokenizer.unk_token_id != tokenizer.eos_token_id:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "        else:\n",
    "            added += tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "    # unkもeosと分離する\n",
    "    if tokenizer.unk_token is None or tokenizer.unk_token_id == tokenizer.eos_token_id:\n",
    "        added += tokenizer.add_special_tokens({'unk_token': '<|unk|>'})\n",
    "\n",
    "    if added > 0:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print('bos:', tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "    print('eos:', tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    print('pad:', tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "    print('unk:', tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "\n",
    "    assert tokenizer.pad_token_id != tokenizer.eos_token_id, 'pad/eos が未分離です'\n",
    "    assert tokenizer.unk_token_id != tokenizer.eos_token_id, 'unk/eos が未分離です'\n",
    "\n",
    "\n",
    "normalize_special_tokens(tokenizer, base_model)\n",
    "\n",
    "\n",
    "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, temperature=0.7, top_p=0.9, repetition_penalty=1.05, do_sample=True):\n",
    "    \"\"\"05ノート専用: pipelineを使わずに安全側でgenerateする。\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        model_device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=False,\n",
    "            )\n",
    "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "def build_prompt(question: str, instruction: str = ''):\n",
    "    body = question if not instruction else f\"{question}\\n{instruction}\"\n",
    "    return (\n",
    "        '以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n'\n",
    "        f'### 指示:\\n{body}\\n\\n### 応答:\\n'\n",
    "    )\n",
    "\n",
    "\n",
    "DEMO_CASES = [\n",
    "    {\n",
    "        'name': 'JSON形式: 主人公情報',\n",
    "        'question': '『星屑のメモリー』の主人公について教えて。',\n",
    "        'instruction': '回答は必ず以下のJSON形式で出力してください。\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
    "    },\n",
    "    {\n",
    "        'name': 'JSON形式: あらすじ要約',\n",
    "        'question': '『古都の探偵録』のあらすじを2文で教えて。',\n",
    "        'instruction': '回答は必ず以下のJSON形式で出力してください。\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
    "    },\n",
    "    {\n",
    "        'name': '要約制約: 2文以内',\n",
    "        'question': '『最後の航海』の事件の概要を教えて。',\n",
    "        'instruction': '回答は2文以内で、固有名詞を1つ以上含めてください。',\n",
    "    },\n",
    "    {\n",
    "        'name': '通常QA: 作品ジャンル',\n",
    "        'question': '『シャドウ・ハンター』はどのようなジャンルの作品ですか？',\n",
    "        'instruction': '',\n",
    "    },\n",
    "    {\n",
    "        'name': '知識外ケース: 安全応答',\n",
    "        'question': '『銀河鉄道999』の2026年版アニメ映画の公式公開日は？',\n",
    "        'instruction': '根拠がない場合は推測せず、「分からない」と明記してください。',\n",
    "    },\n",
    "]\n",
    "\n",
    "# 学習の安定性を優先し、学習前生成はデフォルトでOFF\n",
    "RUN_PRECHECK_BEFORE_TRAIN = False\n",
    "\n",
    "if RUN_PRECHECK_BEFORE_TRAIN:\n",
    "    print('--- 学習前の回答（複数ケース）---')\n",
    "    for i, case in enumerate(DEMO_CASES, start=1):\n",
    "        prompt = build_prompt(case['question'], case['instruction'])\n",
    "        res = safe_generate_local(base_model, tokenizer, prompt, max_new_tokens=128)\n",
    "        answer = res.split('### 応答:')[-1].strip()\n",
    "        print(f\"[{i}] {case['name']}\")\n",
    "        print(f\"Q: {case['question']}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        print('-' * 40)\n",
    "else:\n",
    "    print('学習前生成はスキップします（RUN_PRECHECK_BEFORE_TRAIN=False）。')\n",
    "    print('このまま学習セルへ進んでください。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRA学習の実行 (成果をGoogle Driveへ保存)\n",
    "\n",
    "`PROFILE_NAME` を切り替えて、計算負荷と学習効果のバランスを比較できます。\n",
    "\n",
    "- `quick`: 最短で完走確認（推奨）\n",
    "- `standard`: 演習向け標準設定\n",
    "- `extended`: 時間に余裕がある場合\n",
    "\n",
    "学習成果物は `llm_lab_outputs/lora_runs/<run_name>/final_adapter` に保存され、最新実験のパスは `llm_lab_outputs/selected_adapter_path.txt` に記録されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# 実験プロファイル（必要に応じて PROFILE_NAME を変更）\n",
    "PROFILE_NAME = 'quick'  # quick | standard | extended\n",
    "\n",
    "PROFILES = {\n",
    "    'quick': {\n",
    "        'max_steps': 10,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'max_seq_length': 256,\n",
    "        'learning_rate': 5e-5,\n",
    "    },\n",
    "    'standard': {\n",
    "        'max_steps': 30,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'gradient_accumulation_steps': 8,\n",
    "        'max_seq_length': 512,\n",
    "        'learning_rate': 5e-5,\n",
    "    },\n",
    "    'extended': {\n",
    "        'max_steps': 60,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'gradient_accumulation_steps': 8,\n",
    "        'max_seq_length': 512,\n",
    "        'learning_rate': 5e-5,\n",
    "    },\n",
    "}\n",
    "\n",
    "if PROFILE_NAME not in PROFILES:\n",
    "    raise ValueError(f'PROFILE_NAME must be one of: {list(PROFILES.keys())}')\n",
    "\n",
    "profile = PROFILES[PROFILE_NAME]\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_NAME = f\"{PROFILE_NAME}_s{profile['max_steps']}_lr{profile['learning_rate']}_{timestamp}\"\n",
    "DRIVE_OUTPUT_DIR = os.path.join(DRIVE_RUNS_DIR, RUN_NAME)\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Run name: {RUN_NAME}')\n",
    "print(f'Output dir: {DRIVE_OUTPUT_DIR}')\n",
    "\n",
    "print('1. LoRAアダプタをモデルに追加中...')\n",
    "lora_model = create_lora_model(base_model)\n",
    "\n",
    "print('2. 学習を開始します...')\n",
    "train_lora(\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
    "    output_dir=DRIVE_OUTPUT_DIR,\n",
    "    max_steps=profile['max_steps'],\n",
    "    learning_rate=profile['learning_rate'],\n",
    "    per_device_train_batch_size=profile['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps=profile['gradient_accumulation_steps'],\n",
    "    max_seq_length=profile['max_seq_length'],\n",
    ")\n",
    "\n",
    "FINAL_ADAPTER_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'final_adapter')\n",
    "RUN_CONFIG_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'run_config.json')\n",
    "\n",
    "run_config = {\n",
    "    'run_name': RUN_NAME,\n",
    "    'profile_name': PROFILE_NAME,\n",
    "    'train_dataset_path': 'data/lora/lora_train_sample.jsonl',\n",
    "    'final_adapter_path': FINAL_ADAPTER_PATH,\n",
    "    'params': profile,\n",
    "}\n",
    "\n",
    "with open(RUN_CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(run_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(SELECTED_ADAPTER_RECORD_PATH, 'w', encoding='utf-8') as f:\n",
    "    f.write(FINAL_ADAPTER_PATH + '\\n')\n",
    "\n",
    "print(f'学習完了。アダプタ: {FINAL_ADAPTER_PATH}')\n",
    "print(f'設定保存: {RUN_CONFIG_PATH}')\n",
    "print(f'06向けアダプタ記録: {SELECTED_ADAPTER_RECORD_PATH}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 保存されたアダプタのロードテストと簡易評価\n",
    "\n",
    "Google Drive からアダプタをロードし、動作確認を行います。あわせて少数の評価質問で「JSON形式で返せているか」を簡易チェックします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# アダプタの保存場所（通常は直前セルで生成）\n",
    "DRIVE_ADAPTER_PATH = globals().get('FINAL_ADAPTER_PATH', None)\n",
    "\n",
    "# 念のため selected_adapter_path.txt からも復元可能にする\n",
    "if not DRIVE_ADAPTER_PATH and os.path.exists(SELECTED_ADAPTER_RECORD_PATH):\n",
    "    with open(SELECTED_ADAPTER_RECORD_PATH, 'r', encoding='utf-8') as f:\n",
    "        DRIVE_ADAPTER_PATH = f.read().strip()\n",
    "\n",
    "\n",
    "def build_sanitized_adapter(adapter_path: str):\n",
    "    \"\"\"embed/lm_head を除外した推論用アダプタを /tmp に作る。\"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import tempfile\n",
    "    import torch\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp(prefix='lora_sanitized_')\n",
    "    dst = os.path.join(tmp_dir, 'adapter')\n",
    "    shutil.copytree(adapter_path, dst)\n",
    "\n",
    "    removed = []\n",
    "\n",
    "    safe_path = os.path.join(dst, 'adapter_model.safetensors')\n",
    "    if os.path.exists(safe_path):\n",
    "        from safetensors.torch import load_file, save_file\n",
    "        state = load_file(safe_path, device='cpu')\n",
    "        to_delete = [\n",
    "            k for k in state.keys()\n",
    "            if k.endswith('embed_tokens.weight') or k.endswith('lm_head.weight')\n",
    "        ]\n",
    "        for k in to_delete:\n",
    "            del state[k]\n",
    "        removed.extend(to_delete)\n",
    "        save_file(state, safe_path)\n",
    "\n",
    "    bin_path = os.path.join(dst, 'adapter_model.bin')\n",
    "    if os.path.exists(bin_path):\n",
    "        state = torch.load(bin_path, map_location='cpu')\n",
    "        to_delete = [\n",
    "            k for k in state.keys()\n",
    "            if k.endswith('embed_tokens.weight') or k.endswith('lm_head.weight')\n",
    "        ]\n",
    "        for k in to_delete:\n",
    "            del state[k]\n",
    "        removed.extend(to_delete)\n",
    "        torch.save(state, bin_path)\n",
    "\n",
    "    print(f'Sanitized adapter created: {dst}')\n",
    "    if removed:\n",
    "        print('Removed keys:')\n",
    "        for k in removed:\n",
    "            print(' -', k)\n",
    "    else:\n",
    "        print('No embed/lm_head weights were found in adapter state.')\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "def normalize_for_inference(tokenizer):\n",
    "    \"\"\"推論時の最低限設定。\"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    if '<|endoftext|>' in vocab:\n",
    "        tokenizer.eos_token = '<|endoftext|>'\n",
    "        tokenizer.eos_token_id = vocab['<|endoftext|>']\n",
    "\n",
    "    if tokenizer.bos_token_id == tokenizer.eos_token_id:\n",
    "        tokenizer.bos_token = None\n",
    "        tokenizer.bos_token_id = None\n",
    "\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print('bos:', tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "    print('eos:', tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    print('pad:', tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "def debug_generate(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    try:\n",
    "        print('\\n[Debug] start')\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        model_device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "        prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs.get('attention_mask'),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                min_new_tokens=16,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                remove_invalid_values=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        new_ids = gen_ids[0][prompt_len:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if not text:\n",
    "            print('  decoded text is empty. raw ids:', new_ids[:32].tolist())\n",
    "            raw = tokenizer.decode(new_ids, skip_special_tokens=False).strip()\n",
    "            return raw if raw else '[Empty output]'\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print('\\n[Debug] exception caught:')\n",
    "        print(f'Error type: {type(e).__name__}')\n",
    "        print(f'Error message: {str(e)}')\n",
    "        traceback.print_exc()\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "if DRIVE_ADAPTER_PATH and os.path.exists(DRIVE_ADAPTER_PATH):\n",
    "    print(f'Google Driveからアダプタをロードしています: {DRIVE_ADAPTER_PATH}')\n",
    "\n",
    "    for name in ('lora_model', 'base_model'):\n",
    "        if name in globals() and globals()[name] is not None:\n",
    "            del globals()[name]\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    eval_base_model, eval_tokenizer = load_llm(use_4bit=True)\n",
    "    normalize_for_inference(eval_tokenizer)\n",
    "\n",
    "    sanitized_adapter = build_sanitized_adapter(DRIVE_ADAPTER_PATH)\n",
    "\n",
    "    test_model = PeftModel.from_pretrained(eval_base_model, sanitized_adapter)\n",
    "    test_model.eval()\n",
    "\n",
    "    print('--- 学習後の回答（複数ケース）---')\n",
    "    for i, case in enumerate(DEMO_CASES, start=1):\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"[{i}] {case['name']}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        prompt = build_prompt(case['question'], case['instruction'])\n",
    "        answer = debug_generate(test_model, eval_tokenizer, prompt, max_new_tokens=128)\n",
    "        print(f\"\\nFinal Answer: {answer}\")\n",
    "        print('-' * 60)\n",
    "\n",
    "        if 'Error:' in answer:\n",
    "            print('\\nエラーが発生したため、ここで停止します。')\n",
    "            break\n",
    "else:\n",
    "    print('エラー: Google Drive にアダプタが見つかりません。学習が正常に完了したか確認してください。')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "- LoRAアダプタは `llm_lab_outputs/lora_runs/<run_name>/final_adapter` に保存されます。\n",
    "- 直近で使うアダプタは `llm_lab_outputs/selected_adapter_path.txt` に記録されます。\n",
    "- 次の **06. 統合演習** では、この記録ファイルを使ってアダプタを再利用します。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. LoRAによるファインチューニング (学習と永続化)\n",
        "\n",
        "このノートブックでは、LoRA（QLoRA）による軽量ファインチューニングを行い、その成果物を **Google Driveに実験単位で保存** します。保存したアダプタは **06. 統合演習** で再利用します。\n",
        "\n",
        "## 事前準備\n",
        "Google Colabのメニュー「ランタイム」→「ランタイムのタイプを変更」で **T4 GPU** を選択してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Google Driveのマウント\n",
        "if not os.path.isdir('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "repo_path = '/content/llm_lab'\n",
        "if os.path.exists(repo_path):\n",
        "    !rm -rf {repo_path}\n",
        "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
        "os.chdir(repo_path)\n",
        "\n",
        "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft datasets gradio\n",
        "if 'src' not in sys.path:\n",
        "    sys.path.append(os.path.abspath('src'))\n",
        "\n",
        "from src.common import load_llm, generate_text\n",
        "from src.lora import create_lora_model, train_lora\n",
        "from peft import PeftModel\n",
        "\n",
        "# 05/06で共有する保存先\n",
        "DRIVE_BASE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
        "DRIVE_RUNS_DIR = os.path.join(DRIVE_BASE_DIR, 'lora_runs')\n",
        "SELECTED_ADAPTER_RECORD_PATH = os.path.join(DRIVE_BASE_DIR, 'selected_adapter_path.txt')\n",
        "os.makedirs(DRIVE_RUNS_DIR, exist_ok=True)\n",
        "\n",
        "print('セットアップが完了しました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ベースモデルのロードと学習前の確認\n",
        "\n",
        "単一の質問だけでなく、複数の入力パターンで学習前の挙動を確認します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "base_model, tokenizer = load_llm(use_4bit=True)\n",
        "\n",
        "\n",
        "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, temperature=0.7, top_p=0.9, repetition_penalty=1.05, do_sample=True):\n",
        "    \"\"\"05ノート専用: pipelineを使わずに安全側でgenerateする。\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        model_device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
        "\n",
        "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=False,\n",
        "            )\n",
        "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "def build_prompt(question: str, instruction: str = ''):\n",
        "    body = question if not instruction else f\"{question}\\n{instruction}\"\n",
        "    return (\n",
        "        '以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n'\n",
        "        f'### 指示:\\n{body}\\n\\n### 応答:\\n'\n",
        "    )\n",
        "\n",
        "\n",
        "DEMO_CASES = [\n",
        "    {\n",
        "        'name': 'JSON形式: 主人公情報',\n",
        "        'question': '『星屑のメモリー』の主人公について教えて。',\n",
        "        'instruction': '回答は必ず以下のJSON形式で出力してください。\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
        "    },\n",
        "    {\n",
        "        'name': 'JSON形式: あらすじ要約',\n",
        "        'question': '『古都の探偵録』のあらすじを2文で教えて。',\n",
        "        'instruction': '回答は必ず以下のJSON形式で出力してください。\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
        "    },\n",
        "    {\n",
        "        'name': '要約制約: 2文以内',\n",
        "        'question': '『最後の航海』の事件の概要を教えて。',\n",
        "        'instruction': '回答は2文以内で、固有名詞を1つ以上含めてください。',\n",
        "    },\n",
        "    {\n",
        "        'name': '通常QA: 作品ジャンル',\n",
        "        'question': '『シャドウ・ハンター』はどのようなジャンルの作品ですか？',\n",
        "        'instruction': '',\n",
        "    },\n",
        "    {\n",
        "        'name': '知識外ケース: 安全応答',\n",
        "        'question': '『銀河鉄道999』の2026年版アニメ映画の公式公開日は？',\n",
        "        'instruction': '根拠がない場合は推測せず、「分からない」と明記してください。',\n",
        "    },\n",
        "]\n",
        "\n",
        "# 学習の安定性を優先し、学習前生成はデフォルトでOFF\n",
        "RUN_PRECHECK_BEFORE_TRAIN = False\n",
        "\n",
        "if RUN_PRECHECK_BEFORE_TRAIN:\n",
        "    print('--- 学習前の回答（複数ケース）---')\n",
        "    for i, case in enumerate(DEMO_CASES, start=1):\n",
        "        prompt = build_prompt(case['question'], case['instruction'])\n",
        "        res = safe_generate_local(base_model, tokenizer, prompt, max_new_tokens=128)\n",
        "        answer = res.split('### 応答:')[-1].strip()\n",
        "        print(f\"[{i}] {case['name']}\")\n",
        "        print(f\"Q: {case['question']}\")\n",
        "        print(f\"A: {answer}\")\n",
        "        print('-' * 40)\n",
        "else:\n",
        "    print('学習前生成はスキップします（RUN_PRECHECK_BEFORE_TRAIN=False）。')\n",
        "    print('このまま学習セルへ進んでください。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. LoRA学習の実行 (成果をGoogle Driveへ保存)\n",
        "\n",
        "`PROFILE_NAME` を切り替えて、計算負荷と学習効果のバランスを比較できます。\n",
        "\n",
        "- `quick`: 最短で完走確認（推奨）\n",
        "- `standard`: 演習向け標準設定\n",
        "- `extended`: 時間に余裕がある場合\n",
        "\n",
        "学習成果物は `llm_lab_outputs/lora_runs/<run_name>/final_adapter` に保存され、最新実験のパスは `llm_lab_outputs/selected_adapter_path.txt` に記録されます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "# 実験プロファイル（必要に応じて PROFILE_NAME を変更）\n",
        "PROFILE_NAME = 'quick'  # quick | standard | extended\n",
        "\n",
        "PROFILES = {\n",
        "    'quick': {\n",
        "        'max_steps': 10,\n",
        "        'per_device_train_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 4,\n",
        "        'max_seq_length': 256,\n",
        "        'learning_rate': 5e-5,\n",
        "    },\n",
        "    'standard': {\n",
        "        'max_steps': 30,\n",
        "        'per_device_train_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 8,\n",
        "        'max_seq_length': 512,\n",
        "        'learning_rate': 5e-5,\n",
        "    },\n",
        "    'extended': {\n",
        "        'max_steps': 60,\n",
        "        'per_device_train_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 8,\n",
        "        'max_seq_length': 512,\n",
        "        'learning_rate': 5e-5,\n",
        "    },\n",
        "}\n",
        "\n",
        "if PROFILE_NAME not in PROFILES:\n",
        "    raise ValueError(f'PROFILE_NAME must be one of: {list(PROFILES.keys())}')\n",
        "\n",
        "profile = PROFILES[PROFILE_NAME]\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "RUN_NAME = f\"{PROFILE_NAME}_s{profile['max_steps']}_lr{profile['learning_rate']}_{timestamp}\"\n",
        "DRIVE_OUTPUT_DIR = os.path.join(DRIVE_RUNS_DIR, RUN_NAME)\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f'Run name: {RUN_NAME}')\n",
        "print(f'Output dir: {DRIVE_OUTPUT_DIR}')\n",
        "\n",
        "print('1. LoRAアダプタをモデルに追加中...')\n",
        "lora_model = create_lora_model(base_model)\n",
        "\n",
        "print('2. 学習を開始します...')\n",
        "train_lora(\n",
        "    model=lora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
        "    output_dir=DRIVE_OUTPUT_DIR,\n",
        "    max_steps=profile['max_steps'],\n",
        "    learning_rate=profile['learning_rate'],\n",
        "    per_device_train_batch_size=profile['per_device_train_batch_size'],\n",
        "    gradient_accumulation_steps=profile['gradient_accumulation_steps'],\n",
        "    max_seq_length=profile['max_seq_length'],\n",
        ")\n",
        "\n",
        "FINAL_ADAPTER_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'final_adapter')\n",
        "RUN_CONFIG_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'run_config.json')\n",
        "\n",
        "run_config = {\n",
        "    'run_name': RUN_NAME,\n",
        "    'profile_name': PROFILE_NAME,\n",
        "    'train_dataset_path': 'data/lora/lora_train_sample.jsonl',\n",
        "    'final_adapter_path': FINAL_ADAPTER_PATH,\n",
        "    'params': profile,\n",
        "}\n",
        "\n",
        "with open(RUN_CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(run_config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(SELECTED_ADAPTER_RECORD_PATH, 'w', encoding='utf-8') as f:\n",
        "    f.write(FINAL_ADAPTER_PATH + '\\n')\n",
        "\n",
        "print(f'学習完了。アダプタ: {FINAL_ADAPTER_PATH}')\n",
        "print(f'設定保存: {RUN_CONFIG_PATH}')\n",
        "print(f'06向けアダプタ記録: {SELECTED_ADAPTER_RECORD_PATH}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 保存されたアダプタのロードテストと簡易評価\n",
        "\n",
        "Google Drive からアダプタをロードし、動作確認を行います。あわせて少数の評価質問で「JSON形式で返せているか」を簡易チェックします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "# アダプタの保存場所（通常は直前セルで生成）\n",
        "DRIVE_ADAPTER_PATH = globals().get('FINAL_ADAPTER_PATH', None)\n",
        "\n",
        "# 念のため selected_adapter_path.txt からも復元可能にする\n",
        "if not DRIVE_ADAPTER_PATH and os.path.exists(SELECTED_ADAPTER_RECORD_PATH):\n",
        "    with open(SELECTED_ADAPTER_RECORD_PATH, 'r', encoding='utf-8') as f:\n",
        "        DRIVE_ADAPTER_PATH = f.read().strip()\n",
        "\n",
        "\n",
        "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, temperature=0.7, top_p=0.9, repetition_penalty=1.05, do_sample=True):\n",
        "    \"\"\"05最終セル専用: pipelineを使わずに安全側でgenerateする。\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        model_device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
        "\n",
        "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=False,\n",
        "            )\n",
        "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "if DRIVE_ADAPTER_PATH and os.path.exists(DRIVE_ADAPTER_PATH):\n",
        "    print(f'Google Driveからアダプタをロードしています: {DRIVE_ADAPTER_PATH}')\n",
        "\n",
        "    # 学習安定性を優先: 既存の学習済みlora_modelをそのまま使う（再ロードは重い）\n",
        "    USE_IN_MEMORY_LORA = True\n",
        "\n",
        "    if USE_IN_MEMORY_LORA and 'lora_model' in globals() and lora_model is not None:\n",
        "        test_model = lora_model\n",
        "        eval_tokenizer = tokenizer\n",
        "    else:\n",
        "        # 永続化確認をしたい場合のみ再ロード\n",
        "        eval_base_model, eval_tokenizer = load_llm(use_4bit=True)\n",
        "        test_model = PeftModel.from_pretrained(eval_base_model, DRIVE_ADAPTER_PATH)\n",
        "\n",
        "    test_model.eval()\n",
        "\n",
        "    print('--- 学習後の回答（複数ケース）---')\n",
        "    for i, case in enumerate(DEMO_CASES, start=1):\n",
        "        prompt = build_prompt(case['question'], case['instruction'])\n",
        "        res = safe_generate_local(test_model, eval_tokenizer, prompt, max_new_tokens=128)\n",
        "        answer = res.split('### 応答:')[-1].strip()\n",
        "        print(f\"[{i}] {case['name']}\")\n",
        "        print(f\"Q: {case['question']}\")\n",
        "        print(f\"A: {answer}\")\n",
        "        print('-' * 40)\n",
        "\n",
        "    # 簡易評価: JSON形式の出力率を少数サンプルで確認\n",
        "    eval_path = 'data/lora/lora_eval_questions.json'\n",
        "    if os.path.exists(eval_path):\n",
        "        with open(eval_path, 'r', encoding='utf-8') as f:\n",
        "            eval_data = json.load(f)\n",
        "\n",
        "        max_eval_questions = 3\n",
        "        questions = eval_data.get('questions', [])[:max_eval_questions]\n",
        "        json_ok = 0\n",
        "\n",
        "        print('\\n--- 簡易評価 (JSON形式チェック) ---')\n",
        "        for i, q in enumerate(questions, start=1):\n",
        "            question = q.get('question', '')\n",
        "            eval_instruction = '回答は必ず以下のJSON形式で出力してください。\\\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }'\n",
        "            eval_prompt = build_prompt(question, eval_instruction)\n",
        "\n",
        "            out = safe_generate_local(test_model, eval_tokenizer, eval_prompt, max_new_tokens=128)\n",
        "            answer_text = out.split('### 応答:')[-1].strip()\n",
        "\n",
        "            is_json = False\n",
        "            try:\n",
        "                left = answer_text.find('{')\n",
        "                right = answer_text.rfind('}')\n",
        "                if left != -1 and right != -1 and right > left:\n",
        "                    json.loads(answer_text[left:right + 1])\n",
        "                    is_json = True\n",
        "            except Exception:\n",
        "                is_json = False\n",
        "\n",
        "            json_ok += int(is_json)\n",
        "            print(f'[{i}] JSON形式: {is_json}')\n",
        "            print(f'質問: {question}')\n",
        "            print(f'出力: {answer_text[:180]}')\n",
        "            print('-' * 40)\n",
        "\n",
        "        print(f'JSON形式遵守率: {json_ok}/{len(questions)}')\n",
        "else:\n",
        "    print('エラー: Google Drive にアダプタが見つかりません。学習が正常に完了したか確認してください。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "- LoRAアダプタは `llm_lab_outputs/lora_runs/<run_name>/final_adapter` に保存されます。\n",
        "- 直近で使うアダプタは `llm_lab_outputs/selected_adapter_path.txt` に記録されます。\n",
        "- 次の **06. 統合演習** では、この記録ファイルを使ってアダプタを再利用します。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

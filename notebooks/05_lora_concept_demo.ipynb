{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. LoRAã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (å­¦ç¿’ã¨æ°¸ç¶šåŒ–)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LoRAï¼ˆQLoRAï¼‰ã«ã‚ˆã‚‹è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€ãã®æˆæœç‰©ã‚’ **Google Driveã«å®Ÿé¨“å˜ä½ã§ä¿å­˜** ã—ã¾ã™ã€‚ä¿å­˜ã—ãŸã‚¢ãƒ€ãƒ—ã‚¿ã¯ **06. çµ±åˆæ¼”ç¿’** ã§å†åˆ©ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "## äº‹å‰æº–å‚™\n",
    "Google Colabã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã€â†’ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ã€ã§ **T4 GPU** ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·¨é›†ç¦æ­¢ã‚»ãƒ«\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # CUDAãƒ‡ãƒãƒƒã‚°åŒæœŸå®Ÿè¡Œ\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Google Driveã®ãƒã‚¦ãƒ³ãƒˆ\n",
    "if not os.path.isdir('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if os.path.exists(repo_path):\n",
    "    !rm -rf {repo_path}\n",
    "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "os.chdir(repo_path)\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft datasets gradio\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.common import load_llm, generate_text\n",
    "from src.lora import create_lora_model, train_lora\n",
    "from peft import PeftModel\n",
    "\n",
    "# 05/06ã§å…±æœ‰ã™ã‚‹ä¿å­˜å…ˆ\n",
    "DRIVE_BASE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
    "DRIVE_RUNS_DIR = os.path.join(DRIVE_BASE_DIR, 'lora_runs')\n",
    "SELECTED_ADAPTER_RECORD_PATH = os.path.join(DRIVE_BASE_DIR, 'selected_adapter_path.txt')\n",
    "os.makedirs(DRIVE_RUNS_DIR, exist_ok=True)\n",
    "\n",
    "print('ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨å­¦ç¿’å‰ã®ç¢ºèª\n",
    "\n",
    "å˜ä¸€ã®è³ªå•ã ã‘ã§ãªãã€è¤‡æ•°ã®å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å­¦ç¿’å‰ã®æŒ™å‹•ã‚’ç¢ºèªã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·¨é›†ç¦æ­¢ã‚»ãƒ«\n",
    "base_model, tokenizer = load_llm(use_4bit=True)\n",
    "\n",
    "def normalize_special_tokens(tokenizer, model):\n",
    "    added = 0\n",
    "\n",
    "    # eosã¯æ—¢å­˜ã‚’å„ªå…ˆã—ã€æœªè¨­å®šæ™‚ã®ã¿è¿½åŠ \n",
    "    if tokenizer.eos_token is None:\n",
    "        added += tokenizer.add_special_tokens({'eos_token': '<|eos|>'})\n",
    "\n",
    "    # bosã¯è¨­å®šã—ãªã„ï¼ˆjapanese-stablelmç³»ã§ã¯ä¸è¦ï¼‰\n",
    "    tokenizer.bos_token = None\n",
    "    tokenizer.bos_token_id = None\n",
    "\n",
    "    # padã¯eosã¨åˆ†é›¢ã™ã‚‹\n",
    "    if tokenizer.pad_token is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "        if tokenizer.unk_token is not None and tokenizer.unk_token_id != tokenizer.eos_token_id:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "        else:\n",
    "            added += tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "    # unkã‚‚eosã¨åˆ†é›¢ã™ã‚‹\n",
    "    if tokenizer.unk_token is None or tokenizer.unk_token_id == tokenizer.eos_token_id:\n",
    "        added += tokenizer.add_special_tokens({'unk_token': '<|unk|>'})\n",
    "\n",
    "    if added > 0:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print('bos:', tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "    print('eos:', tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    print('pad:', tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "    print('unk:', tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "\n",
    "    assert tokenizer.pad_token_id != tokenizer.eos_token_id, 'pad/eos ãŒæœªåˆ†é›¢ã§ã™'\n",
    "    assert tokenizer.unk_token_id != tokenizer.eos_token_id, 'unk/eos ãŒæœªåˆ†é›¢ã§ã™'\n",
    "\n",
    "normalize_special_tokens(tokenizer, base_model)\n",
    "\n",
    "\n",
    "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, temperature=0.7, top_p=0.9, repetition_penalty=1.05, do_sample=True):\n",
    "    \"\"\"05ãƒãƒ¼ãƒˆå°‚ç”¨: pipelineã‚’ä½¿ã‚ãšã«å®‰å…¨å´ã§generateã™ã‚‹ã€‚\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        model_device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=False,\n",
    "            )\n",
    "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "def build_prompt(question: str, instruction: str = ''):\n",
    "    body = question if not instruction else f\"{question}\\n{instruction}\"\n",
    "    return (\n",
    "        'ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚\\n\\n'\n",
    "        f'### æŒ‡ç¤º:\\n{body}\\n\\n### å¿œç­”:\\n'\n",
    "    )\n",
    "\n",
    "\n",
    "DEMO_CASES = [\n",
    "    {\n",
    "        'name': 'JSONå½¢å¼: ä¸»äººå…¬æƒ…å ±',\n",
    "        'question': 'ã€æ˜Ÿå±‘ã®ãƒ¡ãƒ¢ãƒªãƒ¼ã€ã®ä¸»äººå…¬ã«ã¤ã„ã¦æ•™ãˆã¦ã€‚',\n",
    "        'instruction': 'å›ç­”ã¯å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
    "    },\n",
    "    {\n",
    "        'name': 'JSONå½¢å¼: ã‚ã‚‰ã™ã˜è¦ç´„',\n",
    "        'question': 'ã€å¤éƒ½ã®æ¢åµéŒ²ã€ã®ã‚ã‚‰ã™ã˜ã‚’2æ–‡ã§æ•™ãˆã¦ã€‚',\n",
    "        'instruction': 'å›ç­”ã¯å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\\n{ \"answer\": \"...\", \"confidence\": \"high|medium|low\" }',\n",
    "    },\n",
    "    {\n",
    "        'name': 'è¦ç´„åˆ¶ç´„: 2æ–‡ä»¥å†…',\n",
    "        'question': 'ã€æœ€å¾Œã®èˆªæµ·ã€ã®äº‹ä»¶ã®æ¦‚è¦ã‚’æ•™ãˆã¦ã€‚',\n",
    "        'instruction': 'å›ç­”ã¯2æ–‡ä»¥å†…ã§ã€å›ºæœ‰åè©ã‚’1ã¤ä»¥ä¸Šå«ã‚ã¦ãã ã•ã„ã€‚',\n",
    "    },\n",
    "    {\n",
    "        'name': 'é€šå¸¸QA: ä½œå“ã‚¸ãƒ£ãƒ³ãƒ«',\n",
    "        'question': 'ã€ã‚·ãƒ£ãƒ‰ã‚¦ãƒ»ãƒãƒ³ã‚¿ãƒ¼ã€ã¯ã©ã®ã‚ˆã†ãªã‚¸ãƒ£ãƒ³ãƒ«ã®ä½œå“ã§ã™ã‹ï¼Ÿ',\n",
    "        'instruction': '',\n",
    "    },\n",
    "    {\n",
    "        'name': 'çŸ¥è­˜å¤–ã‚±ãƒ¼ã‚¹: å®‰å…¨å¿œç­”',\n",
    "        'question': 'ã€éŠ€æ²³é‰„é“999ã€ã®2026å¹´ç‰ˆã‚¢ãƒ‹ãƒ¡æ˜ ç”»ã®å…¬å¼å…¬é–‹æ—¥ã¯ï¼Ÿ',\n",
    "        'instruction': 'æ ¹æ‹ ãŒãªã„å ´åˆã¯æ¨æ¸¬ã›ãšã€ã€Œåˆ†ã‹ã‚‰ãªã„ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚',\n",
    "    },\n",
    "]\n",
    "\n",
    "# å­¦ç¿’ã®å®‰å®šæ€§ã‚’å„ªå…ˆã—ã€å­¦ç¿’å‰ç”Ÿæˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§OFF\n",
    "RUN_PRECHECK_BEFORE_TRAIN = False\n",
    "\n",
    "if RUN_PRECHECK_BEFORE_TRAIN:\n",
    "    print('--- å­¦ç¿’å‰ã®å›ç­”ï¼ˆè¤‡æ•°ã‚±ãƒ¼ã‚¹ï¼‰---')\n",
    "    for i, case in enumerate(DEMO_CASES, start=1):\n",
    "        prompt = build_prompt(case['question'], case['instruction'])\n",
    "        res = safe_generate_local(base_model, tokenizer, prompt, max_new_tokens=128)\n",
    "        answer = res.split('### å¿œç­”:')[-1].strip()\n",
    "        print(f\"[{i}] {case['name']}\")\n",
    "        print(f\"Q: {case['question']}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        print('-' * 40)\n",
    "else:\n",
    "    print('å­¦ç¿’å‰ç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼ˆRUN_PRECHECK_BEFORE_TRAIN=Falseï¼‰ã€‚')\n",
    "    print('ã“ã®ã¾ã¾å­¦ç¿’ã‚»ãƒ«ã¸é€²ã‚“ã§ãã ã•ã„ã€‚')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRAå­¦ç¿’ã®å®Ÿè¡Œ (æˆæœã‚’Google Driveã¸ä¿å­˜)\n",
    "\n",
    "`PROFILE_NAME` ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ã€è¨ˆç®—è² è·ã¨å­¦ç¿’åŠ¹æœã®ãƒãƒ©ãƒ³ã‚¹ã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚\n",
    "\n",
    "- `quick`: æœ€çŸ­ã§å®Œèµ°ç¢ºèªï¼ˆæ¨å¥¨ï¼‰\n",
    "- `standard`: æ¼”ç¿’å‘ã‘æ¨™æº–è¨­å®š\n",
    "- `extended`: æ™‚é–“ã«ä½™è£•ãŒã‚ã‚‹å ´åˆ\n",
    "\n",
    "å­¦ç¿’æˆæœç‰©ã¯ `llm_lab_outputs/lora_runs/<run_name>/final_adapter` ã«ä¿å­˜ã•ã‚Œã€æœ€æ–°å®Ÿé¨“ã®ãƒ‘ã‚¹ã¯ `llm_lab_outputs/selected_adapter_path.txt` ã«è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·¨é›†ç¦æ­¢ã‚»ãƒ«\n",
    "# å®Ÿé¨“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ PROFILE_NAME ã‚’å¤‰æ›´ï¼‰\n",
    "PROFILE_NAME = 'quick'  # quick | standard | extended\n",
    "\n",
    "PROFILES = {\n",
    "    'quick': {\n",
    "        'max_steps': 10,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'max_seq_length': 256,\n",
    "        'learning_rate': 5e-5,\n",
    "    },\n",
    "    'standard': {\n",
    "        'max_steps': 30,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'gradient_accumulation_steps': 8,\n",
    "        'max_seq_length': 512,\n",
    "        'learning_rate': 5e-5,\n",
    "    },\n",
    "    'extended': {\n",
    "        'max_steps': 60,\n",
    "        'per_device_train_batch_size': 1,\n",
    "        'gradient_accumulation_steps': 8,\n",
    "        'max_seq_length': 512,\n",
    "        'learning_rate': 5e-5,\n",
    "    },\n",
    "}\n",
    "\n",
    "if PROFILE_NAME not in PROFILES:\n",
    "    raise ValueError(f'PROFILE_NAME must be one of: {list(PROFILES.keys())}')\n",
    "\n",
    "profile = PROFILES[PROFILE_NAME]\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_NAME = f\"{PROFILE_NAME}_s{profile['max_steps']}_lr{profile['learning_rate']}_{timestamp}\"\n",
    "DRIVE_OUTPUT_DIR = os.path.join(DRIVE_RUNS_DIR, RUN_NAME)\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Run name: {RUN_NAME}')\n",
    "print(f'Output dir: {DRIVE_OUTPUT_DIR}')\n",
    "\n",
    "print('1. LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ä¸­...')\n",
    "lora_model = create_lora_model(base_model)\n",
    "\n",
    "print('2. å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...')\n",
    "train_lora(\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
    "    output_dir=DRIVE_OUTPUT_DIR,\n",
    "    max_steps=profile['max_steps'],\n",
    "    learning_rate=profile['learning_rate'],\n",
    "    per_device_train_batch_size=profile['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps=profile['gradient_accumulation_steps'],\n",
    "    max_seq_length=profile['max_seq_length'],\n",
    ")\n",
    "\n",
    "FINAL_ADAPTER_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'final_adapter')\n",
    "RUN_CONFIG_PATH = os.path.join(DRIVE_OUTPUT_DIR, 'run_config.json')\n",
    "\n",
    "run_config = {\n",
    "    'run_name': RUN_NAME,\n",
    "    'profile_name': PROFILE_NAME,\n",
    "    'train_dataset_path': 'data/lora/lora_train_sample.jsonl',\n",
    "    'final_adapter_path': FINAL_ADAPTER_PATH,\n",
    "    'params': profile,\n",
    "}\n",
    "\n",
    "with open(RUN_CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(run_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(SELECTED_ADAPTER_RECORD_PATH, 'w', encoding='utf-8') as f:\n",
    "    f.write(FINAL_ADAPTER_PATH + '\\n')\n",
    "\n",
    "print(f'å­¦ç¿’å®Œäº†ã€‚ã‚¢ãƒ€ãƒ—ã‚¿: {FINAL_ADAPTER_PATH}')\n",
    "print(f'è¨­å®šä¿å­˜: {RUN_CONFIG_PATH}')\n",
    "print(f'06å‘ã‘ã‚¢ãƒ€ãƒ—ã‚¿è¨˜éŒ²: {SELECTED_ADAPTER_RECORD_PATH}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¿å­˜ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆã¨ç°¡æ˜“è©•ä¾¡\n",
    "\n",
    "Google Drive ã‹ã‚‰ã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€å‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚ã‚ã‚ã›ã¦å°‘æ•°ã®è©•ä¾¡è³ªå•ã§ã€ŒJSONå½¢å¼ã§è¿”ã›ã¦ã„ã‚‹ã‹ã€ã‚’ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·¨é›†ç¦æ­¢ã‚»ãƒ«\n",
    "# ã‚¢ãƒ€ãƒ—ã‚¿ã®ä¿å­˜å ´æ‰€ï¼ˆé€šå¸¸ã¯ç›´å‰ã‚»ãƒ«ã§ç”Ÿæˆï¼‰\n",
    "DRIVE_ADAPTER_PATH = globals().get('FINAL_ADAPTER_PATH', None)\n",
    "\n",
    "# å¿µã®ãŸã‚ selected_adapter_path.txt ã‹ã‚‰ã‚‚å¾©å…ƒå¯èƒ½ã«ã™ã‚‹\n",
    "if not DRIVE_ADAPTER_PATH and os.path.exists(SELECTED_ADAPTER_RECORD_PATH):\n",
    "    with open(SELECTED_ADAPTER_RECORD_PATH, 'r', encoding='utf-8') as f:\n",
    "        DRIVE_ADAPTER_PATH = f.read().strip()\n",
    "\n",
    "\n",
    "def build_sanitized_adapter(adapter_path: str):\n",
    "    \"\"\"embed/lm_head ã‚’é™¤å¤–ã—ãŸæ¨è«–ç”¨ã‚¢ãƒ€ãƒ—ã‚¿ã‚’ /tmp ã«ä½œã‚‹ã€‚\"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import tempfile\n",
    "    import torch\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp(prefix='lora_sanitized_')\n",
    "    dst = os.path.join(tmp_dir, 'adapter')\n",
    "    shutil.copytree(adapter_path, dst)\n",
    "\n",
    "    removed = []\n",
    "\n",
    "    safe_path = os.path.join(dst, 'adapter_model.safetensors')\n",
    "    if os.path.exists(safe_path):\n",
    "        from safetensors.torch import load_file, save_file\n",
    "        state = load_file(safe_path, device='cpu')\n",
    "        to_delete = [\n",
    "            k for k in state.keys()\n",
    "            if k.endswith('embed_tokens.weight') or k.endswith('lm_head.weight')\n",
    "        ]\n",
    "        for k in to_delete:\n",
    "            del state[k]\n",
    "        removed.extend(to_delete)\n",
    "        save_file(state, safe_path)\n",
    "\n",
    "    bin_path = os.path.join(dst, 'adapter_model.bin')\n",
    "    if os.path.exists(bin_path):\n",
    "        state = torch.load(bin_path, map_location='cpu')\n",
    "        to_delete = [\n",
    "            k for k in state.keys()\n",
    "            if k.endswith('embed_tokens.weight') or k.endswith('lm_head.weight')\n",
    "        ]\n",
    "        for k in to_delete:\n",
    "            del state[k]\n",
    "        removed.extend(to_delete)\n",
    "        torch.save(state, bin_path)\n",
    "\n",
    "    print(f'Sanitized adapter created: {dst}')\n",
    "    if removed:\n",
    "        print('Removed keys:')\n",
    "        for k in removed:\n",
    "            print(' -', k)\n",
    "    else:\n",
    "        print('No embed/lm_head weights were found in adapter state.')\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "def normalize_for_inference(tokenizer):\n",
    "    \"\"\"æ¨è«–æ™‚ã®æœ€ä½é™è¨­å®šã€‚\"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    if '<|endoftext|>' in vocab:\n",
    "        tokenizer.eos_token = '<|endoftext|>'\n",
    "        tokenizer.eos_token_id = vocab['<|endoftext|>']\n",
    "\n",
    "    if tokenizer.bos_token_id == tokenizer.eos_token_id:\n",
    "        tokenizer.bos_token = None\n",
    "        tokenizer.bos_token_id = None\n",
    "\n",
    "    # ğŸ”´ ä¿®æ­£ï¼špad_tokenã‚’åˆ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã™ã‚‹\n",
    "    if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "        # unkãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†\n",
    "        if tokenizer.unk_token_id is not None and tokenizer.unk_token_id != tokenizer.eos_token_id:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "        else:\n",
    "            # ãªã‘ã‚Œã°æ–°ã—ã„padãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "            tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "    print('bos:', tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "    print('eos:', tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    print('pad:', tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "\n",
    "def debug_generate(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    try:\n",
    "        print('\\n[Debug] start')\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        model_device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "        prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs.get('attention_mask'),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                min_new_tokens=16,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                remove_invalid_values=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        new_ids = gen_ids[0][prompt_len:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if not text:\n",
    "            print('  decoded text is empty. raw ids:', new_ids[:32].tolist())\n",
    "            raw = tokenizer.decode(new_ids, skip_special_tokens=False).strip()\n",
    "            return raw if raw else '[Empty output]'\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print('\\n[Debug] exception caught:')\n",
    "        print(f'Error type: {type(e).__name__}')\n",
    "        print(f'Error message: {str(e)}')\n",
    "        traceback.print_exc()\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "if DRIVE_ADAPTER_PATH and os.path.exists(DRIVE_ADAPTER_PATH):\n",
    "    print(f'Google Driveã‹ã‚‰ã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™: {DRIVE_ADAPTER_PATH}')\n",
    "\n",
    "    for name in ('lora_model', 'base_model'):\n",
    "        if name in globals() and globals()[name] is not None:\n",
    "            del globals()[name]\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    eval_base_model, eval_tokenizer = load_llm(use_4bit=True)\n",
    "    normalize_for_inference(eval_tokenizer)\n",
    "\n",
    "    sanitized_adapter = build_sanitized_adapter(DRIVE_ADAPTER_PATH)\n",
    "\n",
    "    test_model = PeftModel.from_pretrained(eval_base_model, sanitized_adapter)\n",
    "    test_model.eval()\n",
    "\n",
    "    print('--- å­¦ç¿’å¾Œã®å›ç­”ï¼ˆè¤‡æ•°ã‚±ãƒ¼ã‚¹ï¼‰---')\n",
    "    for i, case in enumerate(DEMO_CASES, start=1):\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"[{i}] {case['name']}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        prompt = build_prompt(case['question'], case['instruction'])\n",
    "        answer = debug_generate(test_model, eval_tokenizer, prompt, max_new_tokens=128)\n",
    "        print(f\"\\nFinal Answer: {answer}\")\n",
    "        print('-' * 60)\n",
    "\n",
    "        if 'Error:' in answer:\n",
    "            print('\\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€ã“ã“ã§åœæ­¢ã—ã¾ã™ã€‚')\n",
    "            break\n",
    "else:\n",
    "    print('ã‚¨ãƒ©ãƒ¼: Google Drive ã«ã‚¢ãƒ€ãƒ—ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ãŸã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "- LoRAã‚¢ãƒ€ãƒ—ã‚¿ã¯ `llm_lab_outputs/lora_runs/<run_name>/final_adapter` ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚\n",
    "- ç›´è¿‘ã§ä½¿ã†ã‚¢ãƒ€ãƒ—ã‚¿ã¯ `llm_lab_outputs/selected_adapter_path.txt` ã«è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚\n",
    "- æ¬¡ã® **06. çµ±åˆæ¼”ç¿’** ã§ã¯ã€ã“ã®è¨˜éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦ã‚¢ãƒ€ãƒ—ã‚¿ã‚’å†åˆ©ç”¨ã—ã¾ã™ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

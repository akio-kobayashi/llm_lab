{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. QLoRAによるファインチューニング実践\n",
    "\n",
    "`05_lora_concept_demo` では、学習済みのLoRAアダプタの効果を確認しました。\n",
    "このノートブックでは、いよいよ自分たちの手で**QLoRA (Quantized LoRA)** の学習を実行します。\n",
    "\n",
    "QLoRAは、4bit量子化されたLLMに対してLoRAチューニングを行う技術で、T4 GPUのようなVRAMが限られた環境でもファインチューニングを可能にします。\n",
    "\n",
    "## この演習のゴール\n",
    "\n",
    "**少量の追加学習データ（10件）を使って短時間のファインチューニングを行い、モデルが特定の出力形式（JSON）をより遵守するようになることを確認する。**\n",
    "\n",
    "成功基準は「JSON形式が壊れにくくなること」であり、回答内容の正しさを追求するものではありません。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "Google Colabで実行する場合、メニューの「ランタイム」→「ランタイムのタイプを変更」で、ハードウェアアクセラレータが「T4 GPU」になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# セットアップの確認と共通モジュールのインポート\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone https://github.com/akio-kobayashi/llm_lab.git\n",
    "os.chdir(repo_path)\n",
    "\n",
    "# 必要なライブラリのインストール\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft trl datasets gradio\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "try:\n",
    "    from src.common import load_llm, generate_text\n",
    "    from src.lora import create_lora_model, train_lora\n",
    "    print('共通モジュールのインポートが完了しました。')\n",
    "except ImportError as e:\n",
    "    print(f'共通モジュールのインポートに失敗しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの準備\n",
    "\n",
    "LoRAの学習には、「指示（input）」と「模範解答（output）」のペアのデータセットが必要です。\n",
    "今回は、`data/lora/lora_train_sample.jsonl` をベースに、**あなた自身で10件の学習データを追加**してもらいます。\n",
    "\n",
    "### 演習: 学習データを追加する\n",
    "\n",
    "以下の `my_additional_data` リストに、10件の学習データを追加してください。\n",
    "**ルール:**\n",
    "- `input` には、モデルへの質問や指示を記述します。\n",
    "- `output` には、その `input` に対する**理想的な**応答を、**厳密なJSON形式**で記述します。\n",
    "- JSONのキーは `{\"answer\": \"...\", \"evidence\": \"...\", \"confidence\": \"...\"}` に統一してください。\n",
    "- `evidence` は、その答えの根拠です。分からなければ `\"該当なし\"` としてください。\n",
    "- `confidence` は、答えの自信度を `\"high\"`, `\"medium\"`, `\"low\"` のいずれかで示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "\n",
    "'''\n",
    "my_additional_data = [\n",
    "    # 例1: 事実に基づく質問\n",
    "    {\"input\": \"日本の現在の総理大臣は誰ですか？\", \"output\": '{\"answer\": \"現在の日本の総理大臣は岸田文雄です。\", \"evidence\": \"一般的な知識\", \"confidence\": \"high\"}'},\n",
    "    \n",
    "    # 例2: 存在しない情報についての質問\n",
    "    {\"input\": \"月にはうさぎが住んでいますか？\", \"output\": '{\"answer\": \"いいえ、月にはうさぎは住んでいません。\", \"evidence\": \"科学的な事実\", \"confidence\": \"high\"}'},\n",
    "\n",
    "    # (1) ここから下に追加してください\n",
    "    {\"input\": \"\", \"output\": ''},\n",
    "    {\"input\": \"\", \"output\": ''},\n",
    "    {\"input\": \"\", \"output\": ''},\n",
    "    {\"input\": \"\", \"output\": ''},\n",
    "    {\"input\": \"\", \"output\": ''},\n",
    "    {\"input\": \"\", \"output\": ''},\n",
    "    {\"input\": \"\", \"output\": ''},\n",
    "    {\"input\": \"\", \"output\": ''}\n",
    "    # (10) ここまで\n",
    "]\n",
    "'''\n",
    "my_additional_data = [\n",
    "    # 例1: 事実に基づく質問\n",
    "    {\"input\": \"日本の現在の総理大臣は誰ですか？\", \"output\": '{\"answer\": \"現在の日本の総理大臣は岸田文雄です。\", \"evidence\": \"一般的な知識\", \"confidence\": \"high\"}'},\n",
    "    # 例2: 存在しない情報についての質問\n",
    "    {\"input\": \"月にはうさぎが住んでいますか？\", \"output\": '{\"answer\": \"いいえ、月にはうさぎは住んでいません。\", \"evidence\": \"科学的な事実\", \"confidence\": \"high\"}'},\n",
    "    # (1) 計算問題\n",
    "    {\"input\": \"12かける5はいくつですか？\", \"output\": '{\"answer\": \"60です。\", \"evidence\": \"数学的計算\", \"confidence\": \"high\"}'},\n",
    "    # (2) 歴史的事実\n",
    "    {\"input\": \"江戸幕府を開いたのは誰ですか？\", \"output\": '{\"answer\": \"徳川家康です。\", \"evidence\": \"日本史\", \"confidence\": \"high\"}'},\n",
    "    # (3) プログラミング/技術\n",
    "    {\"input\": \"Pythonでリストの長さを取得する関数は何ですか？\", \"output\": '{\"answer\": \"len()関数です。\", \"evidence\": \"Python言語仕様\", \"confidence\": \"high\"}'},\n",
    "    # (4) 地理/場所\n",
    "    {\"input\": \"エッフェル塔がある都市はどこですか？\", \"output\": '{\"answer\": \"フランスのパリです。\", \"evidence\": \"地理的データ\", \"confidence\": \"high\"}'},\n",
    "    # (5) 主観的な質問（正解がないもの）\n",
    "    {\"input\": \"世界で一番おいしい果物は何ですか？\", \"output\": '{\"answer\": \"個人の好みによるため、客観的な正解はありません。\", \"evidence\": \"主観的判断\", \"confidence\": \"medium\"}'},\n",
    "    # (6) 前提が誤っている質問（ひっかけ）\n",
    "    {\"input\": \"ペンギンは空を飛びますか？\", \"output\": '{\"answer\": \"いいえ、ペンギンは鳥類ですが空を飛ぶことはできません。\", \"evidence\": \"生物学的特性\", \"confidence\": \"high\"}'},\n",
    "    # (7) 科学的原理\n",
    "    {\"input\": \"水が沸騰するのは摂氏何度ですか？\", \"output\": '{\"answer\": \"標準気圧下では摂氏100度です。\", \"evidence\": \"物理化学的特性\", \"confidence\": \"high\"}'},\n",
    "    # (8) 翻訳/言語\n",
    "    {\"input\": \"「Apple」を日本語に訳すと何ですか？\", \"output\": '{\"answer\": \"「リンゴ」です。\", \"evidence\": \"英和翻訳\", \"confidence\": \"high\"}'}\n",
    "    # (10) ここまで\n",
    "]\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "# 元の学習データと追加データを結合して、新しい学習ファイルを作成\n",
    "base_train_file = 'data/lora/lora_train_sample.jsonl'\n",
    "new_train_file = 'data/lora/lora_train_extended.jsonl'\n",
    "\n",
    "with open(new_train_file, 'w', encoding='utf-8') as f_out:\n",
    "    # 元のデータを書き込む\n",
    "    with open(base_train_file, 'r', encoding='utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            f_out.write(line)\n",
    "    # 追加データを書き込む\n",
    "    for item in my_additional_data:\n",
    "        if item['input'] and item['output']:\n",
    "            f_out.write(json.dumps(item, ensure_ascii=False) + '\n",
    "')\n",
    "\n",
    "print(f'新しい学習データファイルを {new_train_file} として保存しました。')\n",
    "# データ件数の確認\n",
    "!wc -l {new_train_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのロードとLoRAの準備\n",
    "\n",
    "学習のベースとなる4bit量子化モデルをロードし、`create_lora_model` 関数を使ってLoRAアダプタを追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "model, tokenizer = None, None\n",
    "try:\n",
    "    model, tokenizer = load_llm(use_4bit=True)\n",
    "    # LoRAモデルの作成\n",
    "    lora_model = create_lora_model(model)\n",
    "except Exception as e:\n",
    "    print(f'モデルのロードまたはLoRAの準備中にエラーが発生しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習前の性能確認\n",
    "\n",
    "学習を始める前に、現在のモデルがJSON形式の出力をどの程度守れるかを確認しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "question = \"『古都の探偵録』について、作者とあらすじをJSON形式で教えて。\"\n",
    "instruction = \"回答は必ず以下のJSON形式で出力してください。\\n{ \\\"author\\\": \\\"...\\\", \\\"summary\\\": \\\"...\\\" }\"\n",
    "prompt = f\"### 指示:\\n{question}\\n{instruction}\\n\\n### 応答:\\n\"\n",
    "\n",
    "if 'lora_model' in locals() and lora_model:\n",
    "    print(\"--- 学習前の回答 ---\")\n",
    "    generated_text = generate_text(lora_model, tokenizer, prompt, max_new_tokens=128)\n",
    "    answer = generated_text.split(\"### 応答:\")[-\n",
    "1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA学習の実行\n",
    "\n",
    "`train_lora` 関数を呼び出して、QLoRAの学習を開始します。\n",
    "データ量とエポック数を絞っているため、**学習は10〜15分程度**で完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# LoRAアダプタの保存先をGoogle Drive上に設定\n",
    "DRIVE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
    "output_dir = os.path.join(DRIVE_DIR, 'my_lora_adapter')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if 'lora_model' in locals() and lora_model:\n",
    "    trainer = train_lora(\n",
    "        model=lora_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset_path=new_train_file,\n",
    "        output_dir=output_dir,\n",
    "        max_steps=20, # 学習ステップ数（エポック数ではなくステップ数で指定）\n",
    "        per_device_train_batch_size=1, # バッチサイズ\n",
    "        gradient_accumulation_steps=8, # 勾配蓄積数\n",
    "        learning_rate=2e-4, # 学習率\n",
    "        max_seq_length=512 # 扱うトークンの最大長\n",
    "    )\n",
    "else:\n",
    "    print(\"モデルが準備できていないため、学習をスキップします。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習後の性能確認\n",
    "\n",
    "学習が完了したら、学習前と**全く同じ質問**をして、出力がどう変わったかを確認しましょう。\n",
    "うまく学習できていれば、指示されたJSON形式をより忠実に守るようになっているはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "if 'lora_model' in locals() and lora_model:\n",
    "    print(\"--- 学習後の回答 ---\")\n",
    "    # 学習後のモデルで再度テキスト生成\n",
    "    generated_text = generate_text(lora_model, tokenizer, prompt, max_new_tokens=128)\n",
    "    answer = generated_text.split(\"### 応答:\")[-\n",
    "1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、QLoRAによるファインチューニングを実践しました。\n",
    "\n",
    "- **データが重要**: ファインチューニングの性能は、学習データの質と量に大きく依存します。今回は少量でしたが、データ件数を増やし、多様な例を与えることで、より頑健なモデルを作ることができます。\n",
    "- **短時間での適応**: QLoRAにより、限られた計算資源でも、特定のタスクにモデルを効率的に適応させることができました。\n",
    "- **アダプタの保存**: 学習したアダプタは `my_lora_adapter/final_adapter` ディレクトリに保存されています。このアダプタをベースモデルに適用することで、いつでも学習後の性能を再現できます。\n",
    "\n",
    "最後の `07_integrate_gradio.ipynb` では、この演習で学んだRAGとLoRAの技術を組み合わせて、一つのアプリケーションとして完成させます。\n",
    "\n",
    "### メモリ解放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import gc\n",
    "if 'model' in locals() and model is not None: del model\n",
    "if 'lora_model' in locals() and lora_model is not None: del lora_model\n",
    "if 'tokenizer' in locals() and tokenizer is not None: del tokenizer\n",
    "if 'trainer' in locals() and trainer is not None: del trainer\n",
    "model, lora_model, tokenizer, trainer = None, None, None, None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"モデルを解放し、GPUキャッシュをクリアしました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

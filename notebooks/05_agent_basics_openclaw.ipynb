{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; font-size: 0.8em; color: #666;\">\n",
    "最終更新: 2026-02-17 21:30 (Colab Interactive Forms Edition)\n",
    "</div>\n",
    "\n",
    "# 05. LLMベースの軽量マルチエージェント（Executor/Critic）\n",
    "\n",
    "このノートブックでは、LLM（3Bクラス）を使用して、**「実行者（Executor）」**と**「批評者（Critic）」**の2つの役割を連携させる最小構成のエージェントシステムを体験します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. 環境セットアップ (編集禁止)\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "UPDATE_TAG = '20260217_2130'\n",
    "\n",
    "if os.path.exists(repo_path):\n",
    "    !rm -rf {repo_path}\n",
    "\n",
    "!git clone -b ai_agent https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "%cd {repo_path}\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes\n",
    "\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.common import load_llm\n",
    "from src.agent_core import LLMExecutorCriticAgent, RoleConfig\n",
    "\n",
    "print(f'セットアップ完了 (Tag: {UPDATE_TAG})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. モデルのロード (編集禁止)\n",
    "STABLE_MODEL_ID = \"llm-jp/llm-jp-3-3.7b-instruct\"\n",
    "model, tokenizer = load_llm(model_id=STABLE_MODEL_ID, use_4bit=False)\n",
    "\n",
    "def llm_chat(system_prompt: str, user_prompt: str, max_new_tokens: int = 512, temperature: float = 0.2):\n",
    "    try:\n",
    "        prompt = f\"{system_prompt}\\n\\n### 指示:\\n{user_prompt}\\n\\n### 応答:\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048, truncation_side='left')\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "        new_ids = gen_ids[0][prompt_len:]\n",
    "        return tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "    except Exception as e:\n",
    "        return f'Error: {e}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. エージェント設定の調整\n",
    "\n",
    "右側のフォームからパラメータを変更して、エージェントの挙動を調整してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title エージェントの「性格」を設定する { run: \"refer\" }\n",
    "\n",
    "#@markdown ### Executor (実行担当) の設定\n",
    "executor_prompt = \"\\u3042\\u306a\\u305f\\u306f\\u512a\\u79c0\\u306aAI\\u30a2\\u30b7\\u30b9\\u30bf\\u30f3\\u30c8\\u3067\\u3059\\u3002\\u8ad6\\u7406\\u7684\\u304b\\u3064\\u7c21\\u6f54\\u306b\\u56de\\u7b54\\u3057\\u3066\\u304f\\u3060\\u3055\\u3044\\u3002\" #@param {type:\"string\"}\n",
    "executor_temp = 0.3 #@param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "executor_tokens = 500 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ### Critic (批評担当) の設定\n",
    "critic_prompt = \"\\u3042\\u306a\\u305f\\u306f\\u53b3\\u3057\\u3044\\u6279\\u8a55\\u5bb6\\u3067\\u3059\\u3002\\u4e0d\\u5099\\u3084\\u8ad6\\u7406\\u306e\\u98db\\u8e8a\\u3092\\u5bb9\\u820e\\u306a\\u304f\\u6307\\u6458\\u3057\\u3066\\u304f\\u3060\\u3055\\u3044\\u3002\" #@param {type:\"string\"}\n",
    "critic_temp = 0.1 #@param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "critic_tokens = 400 #@param {type:\"number\"}\n",
    "\n",
    "# 設定の適用\n",
    "executor_config = RoleConfig(\n",
    "    name=\"Executor\",\n",
    "    system_prompt=executor_prompt,\n",
    "    temperature=executor_temp,\n",
    "    max_new_tokens=executor_tokens\n",
    ")\n",
    "\n",
    "critic_config = RoleConfig(\n",
    "    name=\"Critic\",\n",
    "    system_prompt=critic_prompt,\n",
    "    temperature=critic_temp,\n",
    "    max_new_tokens=critic_tokens\n",
    ")\n",
    "\n",
    "agent = LLMExecutorCriticAgent(llm_chat, role_configs=[executor_config, critic_config])\n",
    "print(\"エージェントの設定を更新しました。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 動作確認\n",
    "\n",
    "質問を入力して、エージェントを実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 質問の実行 { run: \"auto\" }\n",
    "query = \"Python\\u3067\\u30ea\\u30b9\\u30c8\\u5185\\u306e\\u91cd\\u8907\\u3092\\u524a\\u9664\\u3059\\u308b\\u30b3\\u30fc\\u30c9\\u3092\\u66f8\\u3044\\u3066\\u3002\\u305f\\u3060\\u3057\\u3001\\u8981\\u7d20\\u306e\\u9806\\u756a\\u3092\\u7dad\\u6301\\u3059\\u308b\\u3053\\u3068\\u3092\\u5fd8\\u308c\\u306a\\u3044\\u3067\\u3002\" #@param {type:\"string\"}\n",
    "\n",
    "final_answer, full_log, steps = agent.run_pipeline(query)\n",
    "\n",
    "print('=' * 70)\n",
    "print('質問:', query)\n",
    "print('--- エージェント間のやり取りログ ---')\n",
    "print(full_log)\n",
    "print('=' * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

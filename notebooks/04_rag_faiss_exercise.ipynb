{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Faissを使った本格的なRAGの実装\n",
    "\n",
    "`03_rag_concept_demo` では、手動でコンテキストを与えてRAGの概念を学びました。\n",
    "このノートブックでは、いよいよ本格的なRAGシステムを構築します。\n",
    "\n",
    "具体的には、以下のライブラリを使って「検索」ステップを自動化します。\n",
    "\n",
    "- **Sentence-Transformers**: 文章をベクトル（数値の配列）に変換（Embedding）するためのライブラリ。文章の意味が似ているほど、ベクトル空間上での距離が近くなります。\n",
    "- **Faiss**: Facebook AIが開発した、巨大なベクトルデータの中から高速に類似ベクトルを検索するためのライブラリ。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "Google Colabで実行する場合、メニューの「ランタイム」→「ランタイムのタイプを変更」で、ハードウェアアクセラレータが「T4 GPU」になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# セットアップの確認と共通モジュールのインポート\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git\n",
    "os.chdir(repo_path)\n",
    "\n",
    "# 必要なライブラリのインストール\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft datasets gradio\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "try:\n",
    "    from src.common import load_llm, generate_text\n",
    "    from src.rag import FaissRAGPipeline\n",
    "    print('共通モジュールのインポートが完了しました。')\n",
    "except ImportError:\n",
    "    print('共通モジュールのインポートに失敗しました。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faissインデックスの準備\n",
    "\n",
    "RAGで高速に検索を行うためには、あらかじめ知識源となるドキュメントをすべてベクトル化し、Faissインデックスを構築しておく必要があります。\n",
    "\n",
    "ここでは、`data/docs/anime_docs_sample.jsonl` の内容からインデックスを構築します。\n",
    "（通常は事前に構築済みのインデックスを読み込みますが、今回は学習のため構築プロセスを実行します）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# インデックスとメタデータの保存先パスをGoogle Drive上に設定\n",
    "DRIVE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
    "INDEX_DIR = os.path.join(DRIVE_DIR, 'faiss_index')\n",
    "INDEX_PATH = os.path.join(INDEX_DIR, 'anime_docs.index')\n",
    "META_PATH = os.path.join(INDEX_DIR, 'anime_docs_meta.json')\n",
    "DOCS_PATH = 'data/docs/anime_docs_sample.jsonl'\n",
    "\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "# RAGパイプラインの初期化\n",
    "rag_pipeline = FaissRAGPipeline()\n",
    "\n",
    "# インデックスがまだ存在しない場合のみ、構築・保存する\n",
    "if not os.path.exists(INDEX_PATH):\n",
    "    print('インデックスが存在しないため、新規に構築します。')\n",
    "    rag_pipeline.build_index(DOCS_PATH)\n",
    "    rag_pipeline.save_index(INDEX_PATH, META_PATH)\n",
    "else:\n",
    "    print('既存のインデックスを読み込みます。')\n",
    "    rag_pipeline.load_index(INDEX_PATH, META_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのロード\n",
    "\n",
    "回答生成用のLLMをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "model, tokenizer = None, None\n",
    "try:\n",
    "    model, tokenizer = load_llm(use_4bit=True)\n",
    "except Exception as e:\n",
    "    print(f'モデルのロード中にエラーが発生しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGパイプラインの実行\n",
    "\n",
    "構築したRAGパイプラインを使って、質問応答を試してみましょう。\n",
    "以下の `run_rag_pipeline` 関数は、一連のRAG処理（検索→拡張→生成）をまとめて実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "def run_rag_pipeline(question, top_k=3):\n",
    "    if not all([rag_pipeline, model, tokenizer]):\n",
    "        print(\"RAGパイプラインまたはモデルが初期化されていません。\")\n",
    "        return\n",
    "\n",
    "    # 1. 検索 (Retrieve)\n",
    "    context_docs = rag_pipeline.search(question, top_k=top_k)\n",
    "    \n",
    "    print(\"\\n--- 検索結果 (上位\" + str(top_k) + \"件) ---\")\n",
    "    for i, doc in enumerate(context_docs):\n",
    "        print(f\"[{i+1}] Title: {doc['title']}, Section: {doc['section']}\")\n",
    "        print(f\"   Text: {doc['text'][:100]}...\") # 長すぎるので先頭のみ表示\n",
    "    \n",
    "    # 2. 拡張 (Augment)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(question, context_docs)\n",
    "    \n",
    "    # 3. 生成 (Generate)\n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    answer = generated_text.split(\"回答:\")[-1].strip()\n",
    "    \n",
    "    print(\"\\n--- LLMによる回答 ---\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習1: RAGによる質問応答\n",
    "\n",
    "まずは基本的な質問をしてみましょう。`anime_docs_sample.jsonl` に含まれる情報に基づいて、正しく回答できるはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "question = \"『東京サイバーパンク2042』の主人公はどんな人物ですか？\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "run_rag_pipeline(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習2: `top_k` の値を変更する\n",
    "\n",
    "検索するドキュメントの数 (`top_k`) を変えると、LLMに与えるコンテキストの量が変わります。\n",
    "`top_k` を増やすと、より多くの情報を参考にできますが、ノイズ（関係ない情報）が増える可能性もあります。\n",
    "\n",
    "同じ質問で `top_k` を `1` と `5` に変えて、検索結果と最終的な回答がどう変わるか比較してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "question = \"主人公が料理をするアニメについて教えてください。\"\n",
    "top_k_value = 1 # この値を 5 に変えてみましょう\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "run_rag_pipeline(question, top_k=top_k_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習3: 質問の表現を変える\n",
    "\n",
    "質問の仕方を少し変えるだけで、検索結果（ベクトルの類似度）が変わり、回答に影響することがあります。\n",
    "同じ意図の質問を、異なる表現で試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "# 試行1: 直接的な質問\n",
    "question_1 = \"『星屑のメモリー』のあらすじを教えて。\"\n",
    "\n",
    "# 試行2: 少し曖昧な質問\n",
    "question_2 = \"カイ・ミナトが活躍する物語はどんな話？\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "print(\"--- 試行1 --- \")\n",
    "run_rag_pipeline(question_1, top_k=2)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"--- 試行2 --- \")\n",
    "run_rag_pipeline(question_2, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習4: 失敗例（曖昧な質問や知識ベースにない質問）\n",
    "\n",
    "RAGは万能ではありません。質問が曖昧すぎたり、知識ベースに関連情報が全く存在しない場合は、うまく機能しません。\n",
    "\n",
    "- **曖昧な質問**: 「面白いアニメは？」のような主観的な質問では、どのドキュメントを検索すればよいか特定できません。\n",
    "- **知識ベースにない質問**: 「『星屑のメモリー』の続編は？」のように、`anime_docs_sample.jsonl` に含まれていない情報を質問しても、答えることはできません。\n",
    "\n",
    "これらの「失敗例」を試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "question = \"感動的な物語が見たいです。おすすめはありますか？\"\n",
    "# question = \"『星屑のメモリー』のBlu-rayはいつ発売されますか？\" # こちらも試してみましょう\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "run_rag_pipeline(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、Faissを使って本格的なRAGシステムを構築し、その挙動を観察しました。\n",
    "\n",
    "- RAGは、LLMに外部の事実情報を提供し、ハルシネーションを抑制するのに非常に効果的です。\n",
    "- 検索の質（`top_k`の調整、質問の仕方）が、最終的な回答の質に大きく影響します。\n",
    "- RAGは、あくまで提供された知識ベースの範囲内でしか回答できません。\n",
    "\n",
    "RAGは「事実に基づいた回答」を生成するのに役立ちますが、LLMの「応答スタイル」や「特定のタスクをこなす能力」そのものを変えるわけではありません。\n",
    "例えば、「必ずJSON形式で答えてほしい」といった要求に応えさせるには、プロンプトの工夫だけでは限界があります。\n",
    "\n",
    "この次は、`06_integrate_gradio.ipynb` でRAGをGradio UIに統合し、対話的に検証します。\n",
    "\n",
    "### メモリ解放\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import gc\n",
    "if 'model' in locals() and model is not None: del model\n",
    "if 'tokenizer' in locals() and tokenizer is not None: del tokenizer\n",
    "if 'rag_pipeline' in locals() and rag_pipeline is not None: del rag_pipeline\n",
    "model, tokenizer, rag_pipeline = None, None, None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"リソースを解放し、GPUキャッシュをクリアしました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

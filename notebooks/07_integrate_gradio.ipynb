{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 統合演習: RAGとLoRAを組み合わせたGradio UIの作成\n",
    "\n",
    "このノートブックは、本演習の総まとめです。\n",
    "これまで学んできた技術（LLM, RAG, LoRA）をすべて組み合わせ、一つのアプリケーションを構築します。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "Google Colabで「T4 GPU」になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "repo_path = '/content/llm_lab'\n",
    "\n",
    "# 常に最新の stable-base ブランチを取得する\n",
    "if os.path.exists(repo_path):\n",
    "    !rm -rf {repo_path}\n",
    "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "os.chdir(repo_path)\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft trl datasets gradio\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "import importlib\n",
    "import src.common, src.lora, src.rag, src.ui\n",
    "importlib.reload(src.common)\n",
    "importlib.reload(src.lora)\n",
    "importlib.reload(src.rag)\n",
    "importlib.reload(src.ui)\n",
    "\n",
    "from src.common import load_llm, generate_text\n",
    "from src.rag import FaissRAGPipeline\n",
    "from src.lora import create_lora_model, train_lora\n",
    "from src.ui import create_gradio_ui\n",
    "from peft import PeftModel\n",
    "print('セットアップが完了しました。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAGとモデルの準備\n",
    "\n",
    "インデックスやLoRAアダプタが見つからない場合は、その場で自動作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer, rag_pipeline, lora_model = None, None, None, None\n",
    "\n",
    "try:\n",
    "    # 1. モデルのロード\n",
    "    base_model, tokenizer = load_llm(use_4bit=True)\n",
    "\n",
    "    # 2. RAGの準備\n",
    "    rag_pipeline = FaissRAGPipeline()\n",
    "    sample_docs = 'data/docs/anime_docs_sample.jsonl'\n",
    "    print(\"RAGインデックスを構築しています...\")\n",
    "    rag_pipeline.build_index(sample_docs)\n",
    "\n",
    "    # 3. LoRAアダプタの準備\n",
    "    # リポジトリ内の作業用パスを使用（ドライブ依存を排除）\n",
    "    adapter_dir = \"./data/lora/adapters/demo_adapter\"\n",
    "    adapter_path = os.path.join(adapter_dir, \"final_adapter\")\n",
    "    \n",
    "    if not os.path.exists(adapter_path):\n",
    "        print(\"LoRAアダプタを作成しています（2〜3分）...\")\n",
    "        tmp_lora = create_lora_model(base_model)\n",
    "        train_lora(\n",
    "            model=tmp_lora,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
    "            output_dir=adapter_dir,\n",
    "            max_steps=30,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "        del tmp_lora\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"LoRAアダプタをロードしています: {adapter_path}\")\n",
    "    lora_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    lora_model.eval()\n",
    "    print(\"すべての準備が整いました。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"エラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 生成関数の定義とUI起動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "def generate_plain(q): \n",
    "    return generate_text(base_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\").split(\"### 応答:\")[-1].strip()\n",
    "\n",
    "def generate_rag(q):\n",
    "    docs = rag_pipeline.search(q, top_k=2)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
    "    res = generate_text(base_model, tokenizer, prompt)\n",
    "    ans = res.split(\"回答:\")[-1].strip() if \"回答:\" in res else res.strip()\n",
    "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
    "\n",
    "def generate_lora(q):\n",
    "    res = generate_text(lora_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\")\n",
    "    return res.split(\"### 応答:\")[-1].strip()\n",
    "\n",
    "def generate_rag_lora(q):\n",
    "    docs = rag_pipeline.search(q, top_k=2)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
    "    res = generate_text(lora_model, tokenizer, prompt)\n",
    "    ans = res.split(\"回答:\")[-1].strip() if \"回答:\" in res else res.strip()\n",
    "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
    "\n",
    "sample_queries = [\n",
    "    [\"『星屑のメモリー』の主人公について教えて。\", True, False],\n",
    "    [\"『古都の探偵録』のあらすじを教えて。\", True, True],\n",
    "    [\"日本の首都は？\", False, True],\n",
    "]\n",
    "\n",
    "demo = create_gradio_ui(generate_plain, generate_rag, generate_lora, generate_rag_lora, examples=sample_queries)\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.12" }
 },
 "nbformat": 4, "nbformat_minor": 2
}
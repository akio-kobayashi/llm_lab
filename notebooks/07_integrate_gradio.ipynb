{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 統合演習: RAGとLoRAを組み合わせたGradio UIの作成\n",
    "\n",
    "このノートブックは、本演習の総まとめです。\n",
    "これまで学んできた以下の技術をすべて組み合わせ、一つのインタラクティブなアプリケーションを構築します。\n",
    "\n",
    "- **LLM**: 中核となる言語モデル。\n",
    "- **RAG**: 外部知識を与え、事実に基づいた回答を生成させる技術。\n",
    "- **LoRA**: 特定のタスク（JSON出力）に特化させるためのファインチューニング技術。\n",
    "- **Gradio**: これらの機能を簡単に切り替えられるWeb UI。\n",
    "\n",
    "## この演習のゴール\n",
    "\n",
    "**RAGとLoRAのON/OFFを自由に切り替えられるUIを作成し、それぞれの技術がLLMの応答にどのような影響を与えるかをインタラクティブに体験する。**\n",
    "\n",
    "これにより、「LLMは単なる部品であり、望む結果を得るためには適切なシステム設計が不可欠である」ことを理解します。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "Google Colabで実行する場合、メニューの「ランタイム」→「ランタイムのタイプを変更」で、ハードウェアアクセラレータが「T4 GPU」になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# セットアップの確認と共通モジュールのインポート\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone https://github.com/akio-kobayashi/llm_lab.git\n",
    "os.chdir(repo_path)\n",
    "\n",
    "# 必要なライブラリのインストール\n",
    "!pip install -q -U transformers==4.41.2 accelerate==0.30.1 bitsandbytes==0.43.1 sentence-transformers==2.7.0 faiss-gpu==1.7.2 peft==0.10.0 trl==0.8.6 datasets==2.19.0 gradio==4.31.5\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "try:\n",
    "    from src.common import load_llm, generate_text\n",
    "    from src.rag import FaissRAGPipeline\n",
    "    from src.ui import create_gradio_ui\n",
    "    from peft import PeftModel\n",
    "    print('共通モジュールのインポートが完了しました。')\n",
    "except ImportError as e:\n",
    "    print(f'共通モジュールのインポートに失敗しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAGパイプラインの準備\n",
    "\n",
    "`04_rag_faiss_exercise` と同様に、Faissインデックスを読み込み、RAGパイプラインを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "rag_pipeline = None\n",
    "try:\n",
    "    # Google Drive上のパスを指定\n",
    "    DRIVE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
    "    INDEX_PATH = os.path.join(DRIVE_DIR, 'faiss_index/anime_docs.index')\n",
    "    META_PATH = os.path.join(DRIVE_DIR, 'faiss_index/anime_docs_meta.json')\n",
    "    \n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        rag_pipeline = FaissRAGPipeline()\n",
    "        rag_pipeline.load_index(INDEX_PATH, META_PATH)\n",
    "        print(\"RAGパイプラインの準備が完了しました。\")\n",
    "    else:\n",
    "        print(\"Faissインデックスが見つかりません。04_rag_faiss_exercise.ipynb を先に実行してください。\")\n",
    "except Exception as e:\n",
    "    print(f\"RAGパイプラインの準備中にエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ベースモデルとLoRAモデルの準備\n",
    "\n",
    "ベースとなるLLMと、`06_lora_qlora_exercise` で学習したLoRAアダプタを適用したモデルの両方を準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer, lora_model = None, None, None\n",
    "try:\n",
    "    # ベースモデルのロード\n",
    "    base_model, tokenizer = load_llm(use_4bit=True)\n",
    "    \n",
    "    # Google Drive上のLoRAアダプタのパス\n",
    "    DRIVE_DIR = '/content/drive/MyDrive/llm_lab_outputs'\n",
    "    adapter_path = os.path.join(DRIVE_DIR, 'my_lora_adapter/final_adapter')\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        # ベースモデルにLoRAアダプタをロード\n",
    "        lora_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        lora_model.eval()\n",
    "        print(\"LoRAモデルの準備が完了しました。\")\n",
    "    else:\n",
    "        print(f\"LoRAアダプタが見つかりません: {adapter_path}\")\n",
    "        print(\"LoRAなしモードのみ利用可能です。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'モデルのロード中にエラーが発生しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradio UIのための生成関数を定義\n",
    "\n",
    "UIからのリクエストに応じて、4つの異なるモードでテキストを生成する関数を定義します。\n",
    "\n",
    "1.  **Plain**: 素のベースモデル\n",
    "2.  **RAG**: ベースモデル + RAG\n",
    "3.  **LoRA**: LoRAモデル\n",
    "4.  **RAG + LoRA**: LoRAモデル + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "\n",
    "def generate_plain(query):\n",
    "    prompt = f\"### 指示:\\n{query}\\n\\n### 応答:\\n\"\n",
    "    generated_text = generate_text(base_model, tokenizer, prompt)\n",
    "    return generated_text.split(\"### 応答:\")[-1].strip()\n",
    "\n",
    "def generate_rag(query):\n",
    "    context_docs = rag_pipeline.search(query, top_k=3)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(query, context_docs)\n",
    "    generated_text = generate_text(base_model, tokenizer, prompt)\n",
    "    \n",
    "    answer = generated_text.split(\"回答:\")[-1].strip()\n",
    "    context_str = \"\\n\\n---\\n\\n\".join([json.dumps(doc, ensure_ascii=False, indent=2) for doc in context_docs])\n",
    "    return answer, context_str\n",
    "\n",
    "def generate_lora(query):\n",
    "    if not lora_model:\n",
    "        return \"(LoRAモデルがロードされていません)\", \"\"\n",
    "    prompt = f\"### 指示:\\n{query}\\n\\n### 応答:\\n\"\n",
    "    generated_text = generate_text(lora_model, tokenizer, prompt)\n",
    "    return generated_text.split(\"### 応答:\")[-1].strip()\n",
    "\n",
    "def generate_rag_lora(query):\n",
    "    if not lora_model:\n",
    "        return \"(LoRAモデルがロードされていません)\", \"\"\n",
    "    context_docs = rag_pipeline.search(query, top_k=3)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(query, context_docs)\n",
    "    generated_text = generate_text(lora_model, tokenizer, prompt)\n",
    "    \n",
    "    answer = generated_text.split(\"回答:\")[-1].strip()\n",
    "    context_str = \"\\n\\n---\\n\\n\".join([json.dumps(doc, ensure_ascii=False, indent=2) for doc in context_docs])\n",
    "    return answer, context_str\n",
    "\n",
    "print(\"4つのモードの生成関数を定義しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradio UIの起動\n",
    "\n",
    "準備した関数を使って、`src/ui.py` の `create_gradio_ui` を呼び出し、UIを起動します。\n",
    "セルを実行すると、下にUIが表示されます。`Running on public URL...` のリンクをクリックすると、新しいタブでUIを開くこともできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "if all([base_model, tokenizer, rag_pipeline]):\n",
    "    demo = create_gradio_ui(\n",
    "        generate_func_plain=generate_plain,\n",
    "        generate_func_rag=generate_rag,\n",
    "        generate_func_lora=generate_lora,\n",
    "        generate_func_rag_lora=generate_rag_lora\n",
    "    )\n",
    "    \n",
    "    # share=Trueで外部URLを生成\n",
    "    demo.launch(share=True, debug=True)\n",
    "else:\n",
    "    print(\"必要なコンポーネントがロードされていないため、UIを起動できません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめと考察\n",
    "\n",
    "この演習を通じて、LLMを単体で使うだけでなく、RAGやLoRAといった技術と組み合わせることで、その能力を大きく拡張できることを学びました。\n",
    "\n",
    "- **Plain**: 基本的な対話はできるが、知識が古かったり、嘘をついたりする。\n",
    "- **RAG**: 知識を外部から与えることで、事実に基づいた正確な回答が可能になる。\n",
    "- **LoRA**: 特定のタスク（JSON出力など）に特化させ、応答の「スタイル」を制御できる。\n",
    "- **RAG + LoRA**: 事実に基づき、かつ、望ましいスタイルで応答するという、両方の長所を活かしたシステム。\n",
    "\n",
    "現代のLLMアプリケーション開発では、このように複数の技術を適切に組み合わせ、一つのシステムとして設計することが非常に重要です。\n",
    "\n",
    "これで、本演習のすべての内容は終了です。お疲れ様でした！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 統合演習: RAGとLoRAを組み合わせたGradio UIの作成\n",
    "\n",
    "これまで学んできたLLM、RAG、LoRAをすべて組み合わせ、一つのアプリケーションを構築します。\n",
    "\n",
    "## 事前準備\n",
    "Google Colabで **T4 GPU** になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "# Google Driveのマウント (LoRAアダプタの読み込みに必要)\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if os.path.exists(repo_path): !rm -rf {repo_path}\n",
    "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "os.chdir(repo_path)\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft trl datasets gradio\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.common import load_llm, generate_text\n",
    "from src.rag import FaissRAGPipeline\n",
    "from src.lora import create_lora_model, train_lora\n",
    "from src.ui import create_gradio_ui\n",
    "from peft import PeftModel\n",
    "print('セットアップが完了しました。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAGとモデルの準備\n",
    "\n",
    "Google Driveに保存されたLoRAアダプタをロードします。もし見つからない場合は、その場で簡易作成を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer, rag_pipeline, lora_model = None, None, None, None\n",
    "\n",
    "try:\n",
    "    # 1. モデルのロード\n",
    "    base_model, tokenizer = load_llm(use_4bit=True)\n",
    "\n",
    "    # 2. RAGの準備 (サンプルデータから即時構築)\n",
    "    rag_pipeline = FaissRAGPipeline()\n",
    "    rag_pipeline.build_index('data/docs/anime_docs_sample.jsonl')\n",
    "\n",
    "    # 3. LoRAアダプタのロード (Google Driveから)\n",
    "    adapter_path = \"/content/drive/MyDrive/llm_lab_outputs/demo_adapter/final_adapter\"\n",
    "    \n",
    "    if not os.path.exists(adapter_path):\n",
    "        print(\"Google Driveにアダプタが見つかりません。05を実行していない場合は、その場で作成します（約3分）...\")\n",
    "        tmp_lora = create_lora_model(base_model)\n",
    "        train_lora(\n",
    "            model=tmp_lora,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset_path='data/lora/lora_train_sample.jsonl',\n",
    "            output_dir=\"/content/temp_adapter\",\n",
    "            max_steps=30,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "        adapter_path = \"/content/temp_adapter/final_adapter\"\n",
    "        del tmp_lora\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"LoRAアダプタをロードしています: {adapter_path}\")\n",
    "    lora_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    lora_model.eval()\n",
    "    print(\"すべての準備が整いました。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"エラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UI起動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "def generate_plain(q): \n",
    "    return generate_text(base_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\").split(\"### 応答:\")[-1].strip()\n",
    "\n",
    "def generate_rag(q):\n",
    "    docs = rag_pipeline.search(q, top_k=2)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
    "    res = generate_text(base_model, tokenizer, prompt)\n",
    "    ans = res.split(\"回答:\")[-1].strip() if \"回答:\" in res else res.strip()\n",
    "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
    "\n",
    "def generate_lora(q):\n",
    "    res = generate_text(lora_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\")\n",
    "    return res.split(\"### 応答:\")[-1].strip()\n",
    "\n",
    "def generate_rag_lora(q):\n",
    "    docs = rag_pipeline.search(q, top_k=2)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
    "    res = generate_text(lora_model, tokenizer, prompt)\n",
    "    ans = res.split(\"回答:\")[-1].strip() if \"回答:\" in res else res.strip()\n",
    "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
    "\n",
    "sample_queries = [\n",
    "    [\"『星屑のメモリー』の主人公について教えて。\", True, False],\n",
    "    [\"『古都の探偵録』のあらすじを教えて。\", True, True],\n",
    "    [\"日本の首都は？\", False, True],\n",
    "]\n",
    "\n",
    "demo = create_gradio_ui(generate_plain, generate_rag, generate_lora, generate_rag_lora, examples=sample_queries)\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.12" }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

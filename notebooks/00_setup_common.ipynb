{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. 共通セットアップ\n",
    "\n",
    "このノートブックは、演習全体で必要なライブラリのインストールや設定を行うためのものです。\n",
    "\n",
    "**学習の前に、必ずこのノートブックを一度最後まで実行してください。**\n",
    "\n",
    "## GPUの確認\n",
    "\n",
    "Google ColabでGPUが正しく割り当てられているか確認します。\n",
    "メニューの「ランタイム」→「ランタイムのタイプを変更」で、「ハードウェアアクセラレータ」が「T4 GPU」になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プロジェクトのセットアップ\n",
    "\n",
    "演習用のファイルをGoogle Driveにコピーし、必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# Google Driveをマウント\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# リポジトリをクローン（初回のみ）\n",
    "import os\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone https://github.com/AkkeyLab/llm_lab.git\n",
    "else:\n",
    "    print('Repository already cloned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# 作業ディレクトリへ移動\n",
    "import os\n",
    "os.chdir(repo_path)\n",
    "!ls -F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインストール\n",
    "\n",
    "LLM、RAG、LoRA、Gradioなどを扱うためのライブラリをインストールします。\n",
    "**注意：完了までに5〜10分程度かかります。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "!pip install -q -U transformers==4.41.2\n",
    "!pip install -q -U accelerate==0.30.1\n",
    "!pip install -q -U bitsandbytes==0.43.1\n",
    "!pip install -q -U sentence-transformers==2.7.0\n",
    "!pip install -q -U faiss-gpu==1.7.2 # GPU版FAISS\n",
    "!pip install -q -U peft==0.10.0\n",
    "!pip install -q -U trl==0.8.6\n",
    "!pip install -q -U datasets==2.19.0\n",
    "!pip install -q -U gradio==4.31.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "インストールが完了したら、**一度ランタイムを再起動してください。**\n",
    "\n",
    "メニューの「ランタイム」→「ランタイムを再起動」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共通モジュールのインポートと動作確認\n",
    "\n",
    "ランタイム再起動後、ここから実行を再開します。\n",
    "\n",
    "作成した `src` ディレクトリの共通モジュールをインポートし、正しく動作するか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# Google Driveのマウントとディレクトリ移動（再起動後）\n",
    "import os\n",
    "from google.colab import drive\n",
    "if not os.path.isdir('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if os.path.exists(repo_path):\n",
    "    os.chdir(repo_path)\n",
    "else:\n",
    "    print(f'Error: Directory not found at {repo_path}')\n",
    "\n",
    "# srcディレクトリにパスを通す\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# 共通モジュールのインポート\n",
    "import torch\n",
    "from src.common import load_llm, generate_text\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('GPU available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMのロードとテキスト生成テスト\n",
    "\n",
    "`src/common.py` の `load_llm` 関数を使って、4bit量子化されたLLMをロードします。\n",
    "初回はモデルのダウンロードに時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# モデルのロード（4bit量子化）\n",
    "# メモリを節約するため、このノートブックではテスト後にモデルを解放します\n",
    "try:\n",
    "    model, tokenizer = load_llm(use_4bit=True)\n",
    "except Exception as e:\n",
    "    print(f'モデルのロード中にエラーが発生しました: {e}')\n",
    "    print('GPUメモリが不足している可能性があります。ランタイムを再起動して、このセルから再実行してください。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# テキスト生成のテスト\n",
    "prompt = \"日本で一番高い山はなんですか？\"\n",
    "\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "    print(\"--- プロンプト ---\")\n",
    "    print(prompt)\n",
    "    print(\"\\n--- 生成結果 ---\")\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"モデルがロードされていないため、テキスト生成をスキップしました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# メモリ解放\n",
    "import gc\n",
    "if 'model' in locals(): del model\n",
    "if 'tokenizer' in locals(): del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"モデルを解放し、GPUキャッシュをクリアしました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ完了\n",
    "\n",
    "「生成結果」にテキストが表示されれば、セットアップは正常に完了です。\n",
    "\n",
    "次の `01_gpt_baseline.ipynb` に進んでください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

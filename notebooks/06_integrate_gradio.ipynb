{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. 統合演習: RAGとLoRAを組み合わせたGradio UIの作成\n",
    "\n",
    "## 事前準備\n",
    "Google Colabで **T4 GPU** になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # CUDAデバッグ同期実行\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Google Driveのマウント (05で作成したアダプタの読み込みに必須)\n",
    "if not os.path.isdir('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if os.path.exists(repo_path):\n",
    "    !rm -rf {repo_path}\n",
    "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "os.chdir(repo_path)\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft datasets gradio\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.common import load_llm, generate_text\n",
    "from src.rag import FaissRAGPipeline\n",
    "from src.ui import create_gradio_ui\n",
    "from peft import PeftModel\n",
    "print('セットアップが完了しました。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAGとモデルの準備\n",
    "\n",
    "05 ノートブックで Google Drive に保存した LoRA アダプタをロードします。\n",
    "\n",
    "このノートブックでは **再学習は行いません**。`selected_adapter_path.txt` または `lora_runs` 配下の保存済みアダプタを利用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer, rag_pipeline, lora_model = None, None, None, None\n",
    "\n",
    "\n",
    "def get_adapter_vocab_size(adapter_path: str):\n",
    "    import os\n",
    "    import torch\n",
    "    state = None\n",
    "    safe_path = os.path.join(adapter_path, 'adapter_model.safetensors')\n",
    "    bin_path = os.path.join(adapter_path, 'adapter_model.bin')\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(safe_path):\n",
    "            from safetensors.torch import load_file\n",
    "            state = load_file(safe_path, device='cpu')\n",
    "        elif os.path.exists(bin_path):\n",
    "            state = torch.load(bin_path, map_location='cpu')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if state is None:\n",
    "        return None\n",
    "\n",
    "    for k in [\n",
    "        'base_model.model.model.embed_tokens.weight',\n",
    "        'base_model.model.embed_tokens.weight',\n",
    "        'base_model.model.lm_head.weight',\n",
    "        'base_model.lm_head.weight',\n",
    "    ]:\n",
    "        if k in state and hasattr(state[k], 'shape') and len(state[k].shape) >= 2:\n",
    "            return int(state[k].shape[0])\n",
    "\n",
    "    for k, v in state.items():\n",
    "        if 'embed_tokens.weight' in k and hasattr(v, 'shape') and len(v.shape) >= 2:\n",
    "            return int(v.shape[0])\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_special_tokens_for_inference(tokenizer, model, target_vocab_size=None):\n",
    "    added = 0\n",
    "\n",
    "    def can_add_token():\n",
    "        return target_vocab_size is None or len(tokenizer) < target_vocab_size\n",
    "\n",
    "    if tokenizer.eos_token is None:\n",
    "        if can_add_token():\n",
    "            added += tokenizer.add_special_tokens({'eos_token': '<|eos|>'})\n",
    "        else:\n",
    "            tokenizer.eos_token_id = 0\n",
    "            tokenizer.eos_token = tokenizer.convert_ids_to_tokens(0)\n",
    "\n",
    "    if tokenizer.bos_token_id == tokenizer.eos_token_id:\n",
    "        tokenizer.bos_token = None\n",
    "        tokenizer.bos_token_id = None\n",
    "\n",
    "    if tokenizer.pad_token is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "        if tokenizer.unk_token is not None and tokenizer.unk_token_id != tokenizer.eos_token_id:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "        elif can_add_token():\n",
    "            added += tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "        else:\n",
    "            fallback_id = 1 if tokenizer.eos_token_id != 1 else 2\n",
    "            tokenizer.pad_token_id = fallback_id\n",
    "            tokenizer.pad_token = tokenizer.convert_ids_to_tokens(fallback_id)\n",
    "\n",
    "    if tokenizer.unk_token is None or tokenizer.unk_token_id == tokenizer.eos_token_id:\n",
    "        if can_add_token():\n",
    "            added += tokenizer.add_special_tokens({'unk_token': '<|unk|>'})\n",
    "        else:\n",
    "            fallback_id = 1 if tokenizer.eos_token_id != 1 else 2\n",
    "            tokenizer.unk_token_id = fallback_id\n",
    "            tokenizer.unk_token = tokenizer.convert_ids_to_tokens(fallback_id)\n",
    "\n",
    "    if added > 0:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print('bos:', tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "    print('eos:', tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    print('pad:', tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "    print('unk:', tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "\n",
    "    assert tokenizer.pad_token_id != tokenizer.eos_token_id, 'pad/eos が未分離です'\n",
    "    assert tokenizer.unk_token_id != tokenizer.eos_token_id, 'unk/eos が未分離です'\n",
    "\n",
    "\n",
    "def align_model_tokenizer_to_adapter(model, tokenizer, adapter_vocab_size):\n",
    "    model_vocab = model.get_input_embeddings().weight.shape[0]\n",
    "    tok_vocab = len(tokenizer)\n",
    "    print(f'Before align - Tokenizer: {tok_vocab}, Model: {model_vocab}, Adapter: {adapter_vocab_size}')\n",
    "\n",
    "    normalize_special_tokens_for_inference(tokenizer, model, target_vocab_size=adapter_vocab_size)\n",
    "\n",
    "    tok_vocab = len(tokenizer)\n",
    "    model_vocab = model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "    if adapter_vocab_size is not None:\n",
    "        if tok_vocab < adapter_vocab_size:\n",
    "            extras = [f'<|extra_{i}|>' for i in range(adapter_vocab_size - tok_vocab)]\n",
    "            tokenizer.add_special_tokens({'additional_special_tokens': extras})\n",
    "            tok_vocab = len(tokenizer)\n",
    "            print(f'Tokenizer expanded to: {tok_vocab}')\n",
    "\n",
    "        if tok_vocab > adapter_vocab_size:\n",
    "            raise RuntimeError(\n",
    "                f'Tokenizer vocab ({tok_vocab}) > adapter vocab ({adapter_vocab_size})。'\n",
    "                '学習時と異なるトークン追加が入っています。'\n",
    "            )\n",
    "\n",
    "        if model_vocab != adapter_vocab_size:\n",
    "            model.resize_token_embeddings(adapter_vocab_size)\n",
    "            print(f'Model resized to adapter vocab: {adapter_vocab_size}')\n",
    "    else:\n",
    "        if model_vocab != tok_vocab:\n",
    "            model.resize_token_embeddings(tok_vocab)\n",
    "            print(f'Model resized to tokenizer vocab: {tok_vocab}')\n",
    "\n",
    "    print(f\"After align - eos_id: {tokenizer.eos_token_id}, pad_id: {tokenizer.pad_token_id}\")\n",
    "    print(f\"After align - Model vocab: {model.get_input_embeddings().weight.shape[0]}, Tokenizer vocab: {len(tokenizer)}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # 1. モデルのロード\n",
    "    base_model, tokenizer = load_llm(use_4bit=True)\n",
    "\n",
    "    # 2. RAGの準備 (サンプルデータから即時構築)\n",
    "    rag_pipeline = FaissRAGPipeline()\n",
    "    rag_pipeline.build_index('data/docs/anime_docs_sample.jsonl')\n",
    "\n",
    "    # 3. LoRAアダプタのロード (05で保存した成果物のみ使用)\n",
    "    base_dir = '/content/drive/MyDrive/llm_lab_outputs'\n",
    "    selected_path_record = os.path.join(base_dir, 'selected_adapter_path.txt')\n",
    "    adapter_path = None\n",
    "\n",
    "    if os.path.exists(selected_path_record):\n",
    "        with open(selected_path_record, 'r', encoding='utf-8') as f:\n",
    "            candidate = f.read().strip()\n",
    "        if candidate and os.path.exists(candidate):\n",
    "            adapter_path = candidate\n",
    "\n",
    "    if adapter_path is None:\n",
    "        candidates = glob.glob('/content/drive/MyDrive/llm_lab_outputs/lora_runs/*/final_adapter')\n",
    "        candidates = [p for p in candidates if os.path.exists(p)]\n",
    "        if candidates:\n",
    "            adapter_path = max(candidates, key=os.path.getmtime)\n",
    "\n",
    "    if adapter_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            'LoRAアダプタが見つかりません。05_lora_concept_demo.ipynb を実行してから再試行してください。'\n",
    "        )\n",
    "\n",
    "    adapter_vocab_size = get_adapter_vocab_size(adapter_path)\n",
    "    print(f'Adapter vocab size: {adapter_vocab_size}')\n",
    "    align_model_tokenizer_to_adapter(base_model, tokenizer, adapter_vocab_size)\n",
    "\n",
    "    print(f'LoRAアダプタをロードしています: {adapter_path}')\n",
    "    lora_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    lora_model.eval()\n",
    "    print('すべての準備が整いました。')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'エラーが発生しました: {e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UI起動\n",
    "\n",
    "入力は任意です。JSON指定・要約指定・自由質問など、複数パターンで動作を確認してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "\n",
    "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, repetition_penalty=1.05):\n",
    "    \"\"\"06ノート専用: cache互換問題を避けつつ空出力をフォールバックで回避。\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        model_device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "        prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "        if hasattr(model, 'config'):\n",
    "            model.config.use_cache = False\n",
    "        if hasattr(model, 'generation_config'):\n",
    "            model.generation_config.use_cache = False\n",
    "\n",
    "        base_kwargs = {\n",
    "            **inputs,\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'repetition_penalty': repetition_penalty,\n",
    "            'pad_token_id': pad_token_id,\n",
    "            'eos_token_id': tokenizer.eos_token_id,\n",
    "            'use_cache': False,\n",
    "        }\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen_ids = model.generate(**base_kwargs, do_sample=False)\n",
    "\n",
    "        new_ids = gen_ids[0][prompt_len:]\n",
    "        text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if not text:\n",
    "            with torch.inference_mode():\n",
    "                gen_ids = model.generate(\n",
    "                    **base_kwargs,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.95,\n",
    "                    min_new_tokens=16,\n",
    "                )\n",
    "            new_ids = gen_ids[0][prompt_len:]\n",
    "            text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if not text:\n",
    "            text = tokenizer.decode(new_ids, skip_special_tokens=False).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "def generate_plain(q):\n",
    "    return safe_generate_local(base_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\").strip()\n",
    "\n",
    "\n",
    "def generate_rag(q):\n",
    "    docs = rag_pipeline.search(q, top_k=2)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
    "    ans = safe_generate_local(base_model, tokenizer, prompt)\n",
    "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
    "\n",
    "\n",
    "def generate_lora(q):\n",
    "    return safe_generate_local(lora_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\").strip()\n",
    "\n",
    "\n",
    "def generate_rag_lora(q):\n",
    "    docs = rag_pipeline.search(q, top_k=2)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
    "    ans = safe_generate_local(lora_model, tokenizer, prompt)\n",
    "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
    "\n",
    "\n",
    "sample_queries = [\n",
    "    [\"『星屑のメモリー』の主人公について教えて。\", True, False],\n",
    "    [\"回答はJSON形式で。『古都の探偵録』のあらすじを2文で教えて。\", True, True],\n",
    "    [\"『シャドウ・ハンター』のジャンルを一言で答えて。\", False, True],\n",
    "    [\"日本の首都は？理由を一文で。\", False, True],\n",
    "]\n",
    "\n",
    "\n",
    "demo = create_gradio_ui(generate_plain, generate_rag, generate_lora, generate_rag_lora, examples=sample_queries)\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

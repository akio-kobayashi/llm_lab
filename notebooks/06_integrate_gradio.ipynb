{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06. 統合演習: RAGとLoRAを組み合わせたGradio UIの作成\n",
        "\n",
        "## 事前準備\n",
        "Google Colabで **T4 GPU** になっていることを確認してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Google Driveのマウント (05で作成したアダプタの読み込みに必須)\n",
        "if not os.path.isdir('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "repo_path = '/content/llm_lab'\n",
        "if os.path.exists(repo_path):\n",
        "    !rm -rf {repo_path}\n",
        "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
        "os.chdir(repo_path)\n",
        "\n",
        "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft datasets gradio\n",
        "if 'src' not in sys.path:\n",
        "    sys.path.append(os.path.abspath('src'))\n",
        "\n",
        "from src.common import load_llm, generate_text\n",
        "from src.rag import FaissRAGPipeline\n",
        "from src.ui import create_gradio_ui\n",
        "from peft import PeftModel\n",
        "print('セットアップが完了しました。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. RAGとモデルの準備\n",
        "\n",
        "05 ノートブックで Google Drive に保存した LoRA アダプタをロードします。\n",
        "\n",
        "このノートブックでは **再学習は行いません**。`selected_adapter_path.txt` または `lora_runs` 配下の保存済みアダプタを利用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "base_model, tokenizer, rag_pipeline, lora_model = None, None, None, None\n",
        "\n",
        "try:\n",
        "    # 1. モデルのロード\n",
        "    base_model, tokenizer = load_llm(use_4bit=True)\n",
        "\n",
        "    # 2. RAGの準備 (サンプルデータから即時構築)\n",
        "    rag_pipeline = FaissRAGPipeline()\n",
        "    rag_pipeline.build_index('data/docs/anime_docs_sample.jsonl')\n",
        "\n",
        "    # 3. LoRAアダプタのロード (05で保存した成果物のみ使用)\n",
        "    base_dir = '/content/drive/MyDrive/llm_lab_outputs'\n",
        "    selected_path_record = os.path.join(base_dir, 'selected_adapter_path.txt')\n",
        "    adapter_path = None\n",
        "\n",
        "    if os.path.exists(selected_path_record):\n",
        "        with open(selected_path_record, 'r', encoding='utf-8') as f:\n",
        "            candidate = f.read().strip()\n",
        "        if candidate and os.path.exists(candidate):\n",
        "            adapter_path = candidate\n",
        "\n",
        "    # selected_adapter_path.txt がない場合は、lora_runs配下の最新を補助的に探索\n",
        "    if adapter_path is None:\n",
        "        candidates = glob.glob('/content/drive/MyDrive/llm_lab_outputs/lora_runs/*/final_adapter')\n",
        "        candidates = [p for p in candidates if os.path.exists(p)]\n",
        "        if candidates:\n",
        "            adapter_path = max(candidates, key=os.path.getmtime)\n",
        "\n",
        "    if adapter_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            'LoRAアダプタが見つかりません。05_lora_concept_demo.ipynb を実行してから再試行してください。'\n",
        "        )\n",
        "\n",
        "    print(f'LoRAアダプタをロードしています: {adapter_path}')\n",
        "    lora_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "    lora_model.eval()\n",
        "    print('すべての準備が整いました。')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'エラーが発生しました: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. UI起動\n",
        "\n",
        "入力は任意です。JSON指定・要約指定・自由質問など、複数パターンで動作を確認してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 編集禁止セル\n",
        "\n",
        "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, repetition_penalty=1.05):\n",
        "    \"\"\"06ノート専用: cache互換問題を避けつつ空出力をフォールバックで回避。\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        model_device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
        "        prompt_len = inputs['input_ids'].shape[1]\n",
        "\n",
        "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "        if hasattr(model, 'config'):\n",
        "            model.config.use_cache = False\n",
        "        if hasattr(model, 'generation_config'):\n",
        "            model.generation_config.use_cache = False\n",
        "\n",
        "        base_kwargs = {\n",
        "            **inputs,\n",
        "            'max_new_tokens': max_new_tokens,\n",
        "            'repetition_penalty': repetition_penalty,\n",
        "            'pad_token_id': pad_token_id,\n",
        "            'eos_token_id': tokenizer.eos_token_id,\n",
        "            'use_cache': False,\n",
        "        }\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            gen_ids = model.generate(**base_kwargs, do_sample=False)\n",
        "\n",
        "        new_ids = gen_ids[0][prompt_len:]\n",
        "        text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        if not text:\n",
        "            with torch.inference_mode():\n",
        "                gen_ids = model.generate(\n",
        "                    **base_kwargs,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.9,\n",
        "                    top_p=0.95,\n",
        "                    min_new_tokens=16,\n",
        "                )\n",
        "            new_ids = gen_ids[0][prompt_len:]\n",
        "            text = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        if not text:\n",
        "            text = tokenizer.decode(new_ids, skip_special_tokens=False).strip()\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "def generate_plain(q):\n",
        "    return safe_generate_local(base_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\").strip()\n",
        "\n",
        "\n",
        "def generate_rag(q):\n",
        "    docs = rag_pipeline.search(q, top_k=2)\n",
        "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
        "    ans = safe_generate_local(base_model, tokenizer, prompt)\n",
        "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
        "\n",
        "\n",
        "def generate_lora(q):\n",
        "    return safe_generate_local(lora_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\").strip()\n",
        "\n",
        "\n",
        "def generate_rag_lora(q):\n",
        "    docs = rag_pipeline.search(q, top_k=2)\n",
        "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
        "    ans = safe_generate_local(lora_model, tokenizer, prompt)\n",
        "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n",
        "\n",
        "\n",
        "sample_queries = [\n",
        "    [\"『星屑のメモリー』の主人公について教えて。\", True, False],\n",
        "    [\"回答はJSON形式で。『古都の探偵録』のあらすじを2文で教えて。\", True, True],\n",
        "    [\"『シャドウ・ハンター』のジャンルを一言で答えて。\", False, True],\n",
        "    [\"日本の首都は？理由を一文で。\", False, True],\n",
        "]\n",
        "\n",
        "\n",
        "demo = create_gradio_ui(generate_plain, generate_rag, generate_lora, generate_rag_lora, examples=sample_queries)\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. 統合演習: RAG + Gradio UI\n",
    "\n",
    "このノートブックでは、学習なしで **RAG + Gradio** を統合し、応答品質と根拠表示を確認します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "if not os.path.isdir('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "repo_path = '/content/llm_lab'\n",
    "if os.path.exists(repo_path):\n",
    "    !rm -rf {repo_path}\n",
    "!git clone -b stable-base https://github.com/akio-kobayashi/llm_lab.git {repo_path}\n",
    "os.chdir(repo_path)\n",
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu gradio\n",
    "\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.common import load_llm\n",
    "from src.rag import FaissRAGPipeline\n",
    "from src.ui import create_gradio_ui\n",
    "\n",
    "print('セットアップが完了しました。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "base_model, tokenizer, rag_pipeline = None, None, None\n",
    "\n",
    "base_model, tokenizer = load_llm(use_4bit=True)\n",
    "\n",
    "rag_pipeline = FaissRAGPipeline()\n",
    "rag_pipeline.build_index('data/docs/anime_docs_sample.jsonl')\n",
    "\n",
    "print('モデルとRAGインデックスの準備が完了しました。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import torch\n",
    "\n",
    "def safe_generate_local(model, tokenizer, prompt, max_new_tokens=128, repetition_penalty=1.05):\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        model_device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                remove_invalid_values=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        return tokenizer.decode(gen_ids[0], skip_special_tokens=True).strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def generate_plain(q):\n",
    "    return safe_generate_local(base_model, tokenizer, f\"### 指示:\\n{q}\\n\\n### 応答:\\n\").strip()\n",
    "\n",
    "def generate_rag(q):\n",
    "    docs = rag_pipeline.search(q, top_k=2)\n",
    "    prompt = rag_pipeline.create_prompt_with_context(q, docs)\n",
    "    ans = safe_generate_local(base_model, tokenizer, prompt).strip()\n",
    "    return ans, \"\\n\\n\".join([d['text'] for d in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "demo = create_gradio_ui(\n",
    "    generate_func_plain=generate_plain,\n",
    "    generate_func_rag=generate_rag,\n",
    ")\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. RAGの概念デモ (Retrieval-Augmented Generation)\n",
    "\n",
    "このノートブックでは、LLMのハルシネーション（もっともらしい嘘）を抑制し、事実に基づいた回答を生成させるための強力な技術、**RAG (Retrieval-Augmented Generation)** の基本的な概念を学びます。\n",
    "\n",
    "RAGは、以下の3ステップで構成されます。\n",
    "\n",
    "1.  **検索 (Retrieve)**: ユーザーの質問に関連する情報を、信頼できる知識源（ドキュメントなど）から探し出す。\n",
    "2.  **拡張 (Augment)**: 探し出した情報をプロンプトに埋め込み、元の質問と一緒にLLMに渡す。\n",
    "3.  **生成 (Generate)**: LLMは、与えられた情報（コンテキスト）を**最優先の参考資料として**、質問に対する回答を生成する。\n",
    "\n",
    "これにより、LLMが元々持っていなかった知識についても、正確な回答ができるようになります。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "`00_setup_common.ipynb` を実行して、ライブラリのインストールと設定が完了していることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# セットアップの確認と共通モジュールのインポート\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "if not os.path.isdir('/content/drive'): drive.mount('/content/drive')\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone https://github.com/akio-kobayashi/llm_lab.git\n",
    "os.chdir(repo_path)\n",
    "\n",
    "# 必要なライブラリのインストール\n",
    "!pip install -q -U transformers==4.41.2 accelerate==0.30.1 bitsandbytes==0.43.1 sentence-transformers==2.7.0 faiss-gpu==1.7.2 peft==0.10.0 trl==0.8.6 datasets==2.19.0 gradio==4.31.5\n",
    "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "try:\n",
    "    from src.common import load_llm, generate_text\n",
    "    print('共通モジュールのインポートが完了しました。')\n",
    "except ImportError:\n",
    "    print('共通モジュールのインポートに失敗しました。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのロード\n",
    "\n",
    "演習で共通して使用するLLMをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "model, tokenizer = None, None\n",
    "try:\n",
    "    model, tokenizer = load_llm(use_4bit=True)\n",
    "except Exception as e:\n",
    "    print(f'モデルのロード中にエラーが発生しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGなし vs RAGあり の比較\n",
    "\n",
    "`data/docs/anime_docs_sample.jsonl` に含まれる架空のアニメ『星屑のメモリー』に関する質問をしてみます。\n",
    "この情報はLLMの学習データには含まれていないため、RAGなしでは答えることができません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース1: RAGなし (LLM単体)\n",
    "\n",
    "まずは、これまで通りLLMに直接質問してみます。高い確率でハルシネーションが発生します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "question = \"アニメ『星屑のメモリー』の主人公の名前と、彼が乗る機動兵器の名前を教えてください。\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "if model and tokenizer:\n",
    "    prompt_no_rag = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{question}\\n\\n### 応答:\\n\"\n",
    "    \n",
    "    generated_text = generate_text(model, tokenizer, prompt_no_rag)\n",
    "    \n",
    "    print(\"--- 質問 ---\")\n",
    "    print(question)\n",
    "    print(\"\\n--- RAGなしの回答 ---\")\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース2: RAGあり (手動でコンテキストを付与)\n",
    "\n",
    "次に、RAGの「検索(Retrieve)」と「拡張(Augment)」のステップを**手動で**行ってみます。\n",
    "\n",
    "1.  **検索**: `anime_docs_sample.jsonl` の中から、質問に関係しそうな部分を事前に探し出しておきます。（このセルでは、あらかじめ用意したテキストを `context_document` 変数に入れています）\n",
    "2.  **拡張**: その情報をプロンプトに埋め込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "\n",
    "# 1. 検索(Retrieve)ステップ：手動で関連ドキュメントを用意\n",
    "context_document = \"\"\"\n",
    "doc_id: stardust_memory\n",
    "title: 星屑のメモリー\n",
    "section: 概要\n",
    "text: 『星屑のメモリー』は、銀河連邦とザイオ帝国との間で繰り広げられる壮大な宇宙戦争を描いたSFアニメ作品。主人公の少年カイ・ミナトが、伝説の機動兵器「スターダスト」と出会い、運命の渦に巻き込まれていく物語。友情、裏切り、そして成長をテーマに、緻密なメカニック描写と重厚な人間ドラマが特徴。\n",
    "\"\"\"\n",
    "\n",
    "# 2. 拡張(Augment)ステップ：プロンプトにコンテキストを埋め込む\n",
    "prompt_with_rag = f\"\"\"以下は、参考情報と、それに基づいた質問です。参考情報を使って、質問に正確に答えてください。\n",
    "\n",
    "### 参考情報:\n",
    "{context_document}\n",
    "\n",
    "### 質問:\n",
    "{question}\n",
    "\n",
    "### 応答:\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- RAGで拡張されたプロンプト ---\")\n",
    "print(prompt_with_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この拡張されたプロンプトをLLMに渡して、回答を生成させてみましょう。\n",
    "LLMは「参考情報」の内容を元に回答するため、事実に基づいた正確な答えが返ってくるはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# 3. 生成(Generate)ステップ\n",
    "if model and tokenizer:\n",
    "    generated_text = generate_text(model, tokenizer, prompt_with_rag)\n",
    "    \n",
    "    print(\"--- 質問 ---\")\n",
    "    print(question)\n",
    "    print(\"\\n--- RAGありの回答 ---\")\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、RAGの基本的な概念を学びました。\n",
    "\n",
    "- **RAGなし**: LLMは知らない情報について質問されると、もっともらしい嘘をつく（ハルシネーション）。\n",
    "- **RAGあり**: プロンプトに外部の知識（コンテキスト）を与えることで、LLMはその情報に基づいて正確な回答を生成できる。\n",
    "\n",
    "今回は手動で「検索」を行いましたが、実際のアプリケーションではこの部分を自動化する必要があります。\n",
    "次の `04_rag_faiss_exercise.ipynb` では、`sentence-transformers` と `faiss` というライブラリを使って、質問内容に応じて動的に関連ドキュメントを検索する本格的なRAGシステムを構築します。\n",
    "\n",
    "### メモリ解放\n",
    "次のノートブックに進む前に、GPUメモリを解放します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import gc\n",
    "if 'model' in locals() and model is not None: del model\n",
    "if 'tokenizer' in locals() and tokenizer is not None: del tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"モデルを解放し、GPUキャッシュをクリアしました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

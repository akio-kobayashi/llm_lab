{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. プロンプトエンジニアリングの基礎\n",
    "\n",
    "このノートブックでは、「プロンプト」を工夫することでLLMの振る舞いを制御する基本的なテクニック（プロンプトエンジニアリング）を学びます。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "Google Colabで実行する場合、メニューの「ランタイム」→「ランタイムのタイプを変更」で、ハードウェアアクセラレータが「T4 GPU」になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# セットアップの確認と共通モジュールのインポート\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# Google Driveのマウント\n",
    "if not os.path.isdir('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# リポジトリパスの設定と移動\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone https://github.com/akio-kobayashi/llm_lab.git\n",
    "\n",
    "os.chdir(repo_path)\n",
    "\n",
    "# 必要なライブラリのインストール\n",
    "!pip install -q -U transformers accelerate bitsandbytes sentence-transformers faiss-cpu peft trl datasets gradio\n",
    "\n",
    "# srcディレクトリにパスを通す\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "# 共通関数のインポート\n",
    "try:\n",
    "    from src.common import load_llm, generate_text\n",
    "    print('共通モジュールのインポートが完了しました。')\n",
    "except ImportError:\n",
    "    print('共通モジュールのインポートに失敗しました。00_setup_common.ipynb を実行したか確認してください。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのロード\n",
    "\n",
    "演習で共通して使用するLLMをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "model, tokenizer = None, None\n",
    "try:\n",
    "    model, tokenizer = load_llm(use_4bit=True)\n",
    "except Exception as e:\n",
    "    print(f'モデルのロード中にエラーが発生しました: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プロンプトによる振る舞いの制御\n",
    "\n",
    "LLMへの指示（プロンプト）に、役割（ペルソナ）や制約条件、出力形式などを具体的に記述することで、生成されるテキストをある程度コントロールすることができます。\n",
    "\n",
    "### 演習1: 役割（ペルソナ）を与える\n",
    "\n",
    "LLMに「あなたはプロのツアーコンダクターです」という役割を与えて、回答のスタイルを変化させてみましょう。\n",
    "以下の `persona` と `question` を編集して、どのような変化が起きるか観察してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "persona = \"あなたはプロのツアーコンダクターです。利用者の要望に合わせて、丁寧かつ魅力的な旅行プランを提案してください。\"\n",
    "question = \"家族で楽しめる、夏休みの沖縄旅行プランを考えています。\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "if model and tokenizer:\n",
    "    prompt = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{persona}\\n\\n{question}\\n\\n### 応答:\\n\"\n",
    "    \n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    \n",
    "    print(\"--- プロンプト ---\")\n",
    "    print(prompt)\n",
    "    print(\"\\n--- LLMの回答 ---\")\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習2: 出力形式を指示する\n",
    "\n",
    "回答を特定のフォーマット（例：箇条書き、JSON、Markdownテーブル）で出力するように指示してみましょう。\n",
    "これにより、プログラムで後処理しやすくなります。\n",
    "\n",
    "以下の `output_format_instruction` を編集して、リストや表形式で出力させてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "question = \"日本の有名なアニメ映画を5つ、監督名と公開年と合わせて教えて。\"\n",
    "output_format_instruction = \"回答は必ず以下のMarkdownテーブル形式で出力してください。\\n| 作品名 | 監督名 | 公開年 |\\n|---|---|---|\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "if model and tokenizer:\n",
    "    prompt = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{question}\\n\\n{output_format_instruction}\\n\\n### 応答:\\n\"\n",
    "    \n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    \n",
    "    print(\"--- プロンプト ---\")\n",
    "    print(prompt)\n",
    "    print(\"\\n--- LLMの回答 ---\")\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習3: プロンプトでは知識を増やせないことの確認\n",
    "\n",
    "プロンプトエンジニアリングは、あくまでLLMが**元々持っている知識を引き出す**ためのテクニックです。\n",
    "プロンプトで指示しても、LLMが知らないことは答えられません。\n",
    "\n",
    "`01_gpt_baseline` と同じように、LLMが知らないはずの質問をしてみましょう。\n",
    "丁寧なプロンプトを与えても、結局はもっともらしい嘘（ハルシネーション）を返してしまうことを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "persona = \"あなたは最新のテクノロジーに精通したジャーナリストです。\"\n",
    "question = \"Appleが来月発表すると噂の「iGlass」について、スペックを詳細に教えてください。\"\n",
    "output_format_instruction = \"箇条書きで、スペック項目と予測される性能を記述してください。\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "if model and tokenizer:\n",
    "    prompt = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{persona}\\n\\n{question}\\n\\n{output_format_instruction}\\n\\n### 応答:\\n\"\n",
    "    \n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    \n",
    "    print(\"--- プロンプト ---\")\n",
    "    print(prompt)\n",
    "    print(\"\\n--- LLMの回答 ---\")\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、プロンプトエンジニアリングの基本を学びました。\n",
    "\n",
    "- **振る舞いの制御**: ペルソナや制約を与えることで、LLMの回答スタイルを制御できる。\n",
    "- **出力形式の指定**: 箇条書きやテーブル形式などを指示することで、後処理しやすい形で出力を得られる。\n",
    "- **知識の限界**: プロンプトを工夫しても、LLMが元々知らない知識を創造することはできない。\n",
    "\n",
    "LLMに外部の知識を与えて、事実に基づいた回答をさせるにはどうすればよいでしょうか？\n",
    "その答えが、次の `03_rag_concept_demo.ipynb` で学ぶ **RAG (Retrieval-Augmented Generation)** です。\n",
    "\n",
    "### メモリ解放\n",
    "次のノートブックに進む前に、GPUメモリを解放します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import gc\n",
    "if 'model' in locals() and model is not None: del model\n",
    "if 'tokenizer' in locals() and tokenizer is not None: del tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"モデルを解放し、GPUキャッシュをクリアしました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

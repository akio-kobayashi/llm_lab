{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. LLM単体でのテキスト生成 (ベースライン)\n",
    "\n",
    "このノートブックでは、ファインチューニングやRAGを行っていない、素のLLM（大規模言語モデル）がどのようにテキストを生成するかを体験します。\n",
    "\n",
    "## 事前準備\n",
    "\n",
    "Google Colabで実行する場合、メニューの「ランタイム」→「ランタイムのタイプを変更」で、ハードウェアアクセラレータが「T4 GPU」になっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# セットアップの確認と共通モジュールのインポート\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# Google Driveのマウント\n",
    "if not os.path.isdir('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# リポジトリパスの設定と移動\n",
    "repo_path = '/content/llm_lab'\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone https://github.com/akio-kobayashi/llm_lab.git\n",
    "\n",
    "os.chdir(repo_path)\n",
    "\n",
    "# 必要なライブラリのインストール（Colab環境用）\n",
    "print('ライブラリをインストールしています...（数分かかります）')\n",
    "!pip install -q -U transformers==4.41.2 accelerate==0.30.1 bitsandbytes==0.46.1 sentence-transformers==2.7.0 faiss-gpu peft==0.10.0 trl==0.8.6 datasets==2.19.0 gradio==4.31.5\n",
    "\n",
    "\n",
    "# srcディレクトリにパスを通す\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "# 共通関数のインポート\n",
    "try:\n",
    "    from src.common import load_llm, generate_text\n",
    "    print('共通モジュールのインポートが完了しました。')\n",
    "except ImportError:\n",
    "    print('共通モジュールのインポートに失敗しました。00_setup_common.ipynb を実行したか確認してください。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのロード\n",
    "\n",
    "演習で共通して使用するLLMをロードします。`common.py` で定義された `load_llm` 関数を呼び出します。\n",
    "モデルのロードには数分かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "# グローバル変数としてモデルとトークナイザを保持\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model_globally():\n",
    "    global model, tokenizer\n",
    "    if model is None or tokenizer is None:\n",
    "        try:\n",
    "            model, tokenizer = load_llm(use_4bit=True)\n",
    "        except Exception as e:\n",
    "            print(f'モデルのロード中にエラーが発生しました: {e}')\n",
    "            print('GPUメモリが不足している可能性があります。ランタイムを再起動して、このセルから再実行してください。')\n",
    "    else:\n",
    "        print(\"モデルは既にロードされています。\")\n",
    "\n",
    "load_model_globally()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成の体験\n",
    "\n",
    "ロードしたLLMに様々な質問を投げかけて、どのような回答が返ってくるかを見てみましょう。\n",
    "\n",
    "### 演習1: 同じ質問を繰り返す\n",
    "\n",
    "以下のセルの `question` に好きな質問を入れて、**複数回実行**してみてください。\n",
    "毎回、少しずつ回答内容が変わる（＝生成が揺れる）ことを確認しましょう。これは、LLMが確率に基づいて次の単語を予測しているために起こる現象です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "question = \"日本のおすすめの観光地を3つ教えてください。\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "if model and tokenizer:\n",
    "    # プロンプトテンプレートを適用\n",
    "    prompt = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{question}\\n\\n### 応答:\\n\"\n",
    "    \n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    \n",
    "    print(\"--- 質問 ---\")\n",
    "    print(question)\n",
    "    print(\"\\n--- LLMの回答 ---\")\n",
    "    # 応答部分のみを切り出して表示\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習2: LLMが知らないはずの質問をする\n",
    "\n",
    "LLMは、学習データに含まれていない、非常に新しい情報や、架空の情報については答えることができません。\n",
    "しかし、それらしい嘘の回答（**ハルシネーション**）を生成してしまうことがあります。\n",
    "\n",
    "以下の `question` に、LLMが知らないであろう質問（例：昨日の夕食、来年の流行語、この演習の作者のフルネームなど）を入力して、どのような反応をするか観察してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ここを編集 --- #\n",
    "question = \"2040年のワールドカップはどこで開催されますか？\"\n",
    "# --- 編集ここまで --- #\n",
    "\n",
    "# 編集禁止セル\n",
    "if model and tokenizer:\n",
    "    prompt = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n{question}\\n\\n### 応答:\\n\"\n",
    "    \n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    \n",
    "    print(\"--- 質問 ---\")\n",
    "    print(question)\n",
    "    print(\"\\n--- LLMの回答 ---\")\n",
    "    answer = generated_text.split(\"### 応答:\")[-1].strip()\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"モデルがロードされていません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、素のLLMの基本的な性質を学びました。\n",
    "\n",
    "- **回答の揺れ**: 同じ質問でも、実行するたびに異なる回答が生成されることがある。\n",
    "- **ハルシネーション**: 事実に基づかない、もっともらしい嘘の情報を生成することがある。\n",
    "\n",
    "これらの性質は、LLMをシステムに組み込む際に考慮すべき重要な課題です。\n",
    "次の `02_prompting.ipynb` では、プロンプトを工夫することで、これらの振る舞いをある程度制御する方法を学びます。\n",
    "\n",
    "### メモリ解放\n",
    "次のノートブックに進む前に、GPUメモリを解放します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編集禁止セル\n",
    "import gc\n",
    "if 'model' in globals() and model is not None: del model\n",
    "if 'tokenizer' in globals() and tokenizer is not None: del tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"モデルを解放し、GPUキャッシュをクリアしました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
